{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e7fedd",
   "metadata": {},
   "source": [
    "# Deterministic Codec — Lightweight Round-Trip Tests\n",
    "\n",
    "This notebook is intentionally **safe/lightweight**:\n",
    "\n",
    "- Uses a tiny test model (`sshleifer/tiny-gpt2`) by default.\n",
    "- Uses short texts only.\n",
    "- Caps decode steps.\n",
    "- Runs CPU-first tests before any optional cross-device check.\n",
    "- Keeps cross-device test **disabled by default**.\n",
    "\n",
    "If you want to be extra safe, keep `RUN_CROSS_DEVICE = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b205e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety knobs (edit if needed)\n",
    "MODEL_ID = \"deepseek-ai/deepseek-coder-1.3b-base\"   # keep tiny by default for safe tests\n",
    "MAX_TEXT_CHARS = 128\n",
    "MAX_DECODE_TOKENS = 256\n",
    "MAX_TEST_TIMEOUT_SEC = 45\n",
    "\n",
    "# Extra-strict limits for cross-device smoke test\n",
    "CROSS_DEVICE_MAX_DECODE_TOKENS = 64\n",
    "CROSS_DEVICE_TIMEOUT_SEC = 20\n",
    "\n",
    "RUN_STRICT_CPU = True\n",
    "RUN_GPU_BEST_EFFORT = True\n",
    "RUN_CROSS_DEVICE = True  # set False to skip cross-device checks\n",
    "\n",
    "# Optional device targets for cross-device cell\n",
    "ENCODE_DEVICE = \"cpu\"\n",
    "DECODE_DEVICE = \"mps\"  # change to \"cuda\" if needed and available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f66592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Utils] Success: Loaded C++ Exact Softmax Kernel.\n",
      "torch: 2.8.0\n",
      "cuda available: False\n",
      "mps available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import signal\n",
    "import traceback\n",
    "import importlib\n",
    "import torch\n",
    "\n",
    "# Avoid tokenizers fork-parallelism warning / potential deadlock scenarios\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import deterministic_runtime\n",
    "import llm_codec_deterministic\n",
    "importlib.reload(deterministic_runtime)\n",
    "importlib.reload(llm_codec_deterministic)\n",
    "\n",
    "DeterministicCodecConfig = llm_codec_deterministic.DeterministicCodecConfig\n",
    "DeterministicLLMCodec = llm_codec_deterministic.DeterministicLLMCodec\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"mps available:\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1753e108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: deepseek-ai/deepseek-coder-1.3b-base\n"
     ]
    }
   ],
   "source": [
    "# Load tiny model once (safe-ish); catches network/download issues gracefully\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
    "    print(f\"Loaded model: {MODEL_ID}\")\n",
    "except Exception as e:\n",
    "    tokenizer = None\n",
    "    model = None\n",
    "    print(\"Model load failed. Details:\")\n",
    "    print(type(e).__name__, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63ee478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_with_timeout(fn, timeout_sec, *args, **kwargs):\n",
    "    if timeout_sec is None or timeout_sec <= 0:\n",
    "        return fn(*args, **kwargs)\n",
    "\n",
    "    if os.name != \"posix\":\n",
    "        # Fallback: no hard wall-clock timeout on non-posix systems\n",
    "        return fn(*args, **kwargs)\n",
    "\n",
    "    class _NotebookTimeoutError(TimeoutError):\n",
    "        pass\n",
    "\n",
    "    def _handler(signum, frame):\n",
    "        raise _NotebookTimeoutError(f\"Timed out after {timeout_sec}s\")\n",
    "\n",
    "    old_handler = signal.getsignal(signal.SIGALRM)\n",
    "    signal.signal(signal.SIGALRM, _handler)\n",
    "    signal.setitimer(signal.ITIMER_REAL, float(timeout_sec))\n",
    "    try:\n",
    "        return fn(*args, **kwargs)\n",
    "    finally:\n",
    "        signal.setitimer(signal.ITIMER_REAL, 0.0)\n",
    "        signal.signal(signal.SIGALRM, old_handler)\n",
    "\n",
    "\n",
    "def tiny_roundtrip_test(\n",
    "    text,\n",
    "    mode=\"strict_cpu\",\n",
    "    encode_device=\"cpu\",\n",
    "    decode_device=\"cpu\",\n",
    "    max_decode_tokens=None,\n",
    "    timeout_sec=None,\n",
    "):\n",
    "    if tokenizer is None or model is None:\n",
    "        return {\"ok\": False, \"reason\": \"model_not_loaded\", \"mode\": mode}\n",
    "\n",
    "    text = text[:MAX_TEXT_CHARS]\n",
    "    decode_cap = MAX_DECODE_TOKENS if max_decode_tokens is None else int(max_decode_tokens)\n",
    "    test_timeout = MAX_TEST_TIMEOUT_SEC if timeout_sec is None else float(timeout_sec)\n",
    "\n",
    "    cfg = DeterministicCodecConfig(\n",
    "        determinism_mode=mode,\n",
    "        precision=32,\n",
    "        slots=(1 << 20),  # smaller than production for faster/safe smoke tests\n",
    "        use_legacy_counts=True,\n",
    "        use_kv_cache=False,\n",
    "        patch_linear=True,\n",
    "        patch_rmsnorm=True,\n",
    "        patch_attention=True,\n",
    "    )\n",
    "\n",
    "    started = time.time()\n",
    "    try:\n",
    "        encoder = DeterministicLLMCodec(tokenizer, model, device=encode_device, config=cfg)\n",
    "        encoded = _run_with_timeout(encoder.encode, test_timeout, text)\n",
    "\n",
    "        decoder = DeterministicLLMCodec(tokenizer, model, device=decode_device, config=cfg)\n",
    "        decoded = _run_with_timeout(decoder.decode, test_timeout, encoded, decode_cap)\n",
    "\n",
    "        return {\n",
    "            \"ok\": decoded == text,\n",
    "            \"mode\": mode,\n",
    "            \"encode_device\": encode_device,\n",
    "            \"decode_device\": decode_device,\n",
    "            \"input_len\": len(text),\n",
    "            \"encoded_bytes\": len(encoded),\n",
    "            \"decode_cap\": decode_cap,\n",
    "            \"timeout_sec\": test_timeout,\n",
    "            \"elapsed_sec\": round(time.time() - started, 3),\n",
    "            \"decoded_preview\": decoded[:80],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"ok\": False,\n",
    "            \"mode\": mode,\n",
    "            \"encode_device\": encode_device,\n",
    "            \"decode_device\": decode_device,\n",
    "            \"decode_cap\": decode_cap,\n",
    "            \"timeout_sec\": test_timeout,\n",
    "            \"error\": f\"{type(e).__name__}: {e}\",\n",
    "            \"traceback\": traceback.format_exc().splitlines()[-6:],\n",
    "            \"elapsed_sec\": round(time.time() - started, 3),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9070400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deterministic Encode: 100%|██████████| 3/3 [00:01<00:00,  1.91it/s]\n",
      "Deterministic Encode: 100%|██████████| 14/14 [00:07<00:00,  1.92it/s]\n",
      "Deterministic Encode: 100%|██████████| 15/15 [00:07<00:00,  1.92it/s]\n",
      "Deterministic Encode: 100%|██████████| 3/3 [00:00<00:00,  5.93it/s]\n",
      "Deterministic Encode: 100%|██████████| 14/14 [00:02<00:00,  5.13it/s]\n",
      "Deterministic Encode: 100%|██████████| 15/15 [00:02<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ok': True, 'mode': 'strict_cpu', 'encode_device': 'cpu', 'decode_device': 'cpu', 'input_len': 5, 'encoded_bytes': 8, 'elapsed_sec': 3.031, 'decoded_preview': 'hello'}\n",
      "{'ok': True, 'mode': 'strict_cpu', 'encode_device': 'cpu', 'decode_device': 'cpu', 'input_len': 44, 'encoded_bytes': 9, 'elapsed_sec': 14.576, 'decoded_preview': 'The quick brown fox jumps over the lazy dog.'}\n",
      "{'ok': True, 'mode': 'strict_cpu', 'encode_device': 'cpu', 'decode_device': 'cpu', 'input_len': 47, 'encoded_bytes': 14, 'elapsed_sec': 15.616, 'decoded_preview': 'Deterministic codecs should round-trip exactly.'}\n",
      "{'ok': True, 'mode': 'gpu_best_effort', 'encode_device': 'cpu', 'decode_device': 'cpu', 'input_len': 5, 'encoded_bytes': 8, 'elapsed_sec': 1.026, 'decoded_preview': 'hello'}\n",
      "{'ok': True, 'mode': 'gpu_best_effort', 'encode_device': 'cpu', 'decode_device': 'cpu', 'input_len': 44, 'encoded_bytes': 9, 'elapsed_sec': 5.483, 'decoded_preview': 'The quick brown fox jumps over the lazy dog.'}\n",
      "{'ok': True, 'mode': 'gpu_best_effort', 'encode_device': 'cpu', 'decode_device': 'cpu', 'input_len': 47, 'encoded_bytes': 14, 'elapsed_sec': 5.981, 'decoded_preview': 'Deterministic codecs should round-trip exactly.'}\n",
      "\n",
      "ALL LOCAL TESTS PASSED: True\n"
     ]
    }
   ],
   "source": [
    "# Lightweight local tests (CPU only)\n",
    "texts = [\n",
    "    \"hello\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Deterministic codecs should round-trip exactly.\",\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "if RUN_STRICT_CPU:\n",
    "    for t in texts:\n",
    "        results.append(tiny_roundtrip_test(t, mode=\"strict_cpu\", encode_device=\"cpu\", decode_device=\"cpu\"))\n",
    "\n",
    "if RUN_GPU_BEST_EFFORT:\n",
    "    # still CPU devices here to keep this cell lightweight and stable\n",
    "    for t in texts:\n",
    "        results.append(tiny_roundtrip_test(t, mode=\"gpu_best_effort\", encode_device=\"cpu\", decode_device=\"cpu\"))\n",
    "\n",
    "for r in results:\n",
    "    print(r)\n",
    "\n",
    "all_ok = all(r.get(\"ok\", False) for r in results) if results else False\n",
    "print(\"\\nALL LOCAL TESTS PASSED:\", all_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f401cfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deterministic Encode: 100%|██████████| 9/9 [00:04<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ok': False, 'mode': 'strict_cpu', 'encode_device': 'cpu', 'decode_device': 'mps', 'decode_cap': 64, 'timeout_sec': 20.0, 'error': '_NotebookTimeoutError: Timed out after 20.0s', 'traceback': ['  File \"/Users/wuxidami/LLM_Text_Compression_IIB_Project/deterministic_runtime.py\", line 107, in deterministic_matmul', '    return out.to(device=a.device, dtype=a.dtype)', '           ~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '  File \"/var/folders/9q/kr6k165j47b7qv4mlqdh4_9r0000gn/T/ipykernel_23277/1214143450.py\", line 13, in _handler', '    raise _NotebookTimeoutError(f\"Timed out after {timeout_sec}s\")', '_run_with_timeout.<locals>._NotebookTimeoutError: Timed out after 20.0s'], 'elapsed_sec': 25.756}\n"
     ]
    }
   ],
   "source": [
    "# Optional cross-device smoke test (guarded with strict decode/time limits)\n",
    "if RUN_CROSS_DEVICE:\n",
    "    short_text = \"Cross-device tiny smoke test.\"\n",
    "    cross_res = tiny_roundtrip_test(\n",
    "        short_text,\n",
    "        mode=\"strict_cpu\",\n",
    "        encode_device=ENCODE_DEVICE,\n",
    "        decode_device=DECODE_DEVICE,\n",
    "        max_decode_tokens=CROSS_DEVICE_MAX_DECODE_TOKENS,\n",
    "        timeout_sec=CROSS_DEVICE_TIMEOUT_SEC,\n",
    "    )\n",
    "    print(cross_res)\n",
    "else:\n",
    "    print(\"RUN_CROSS_DEVICE is False; skipped cross-device test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76137da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deterministic Encode: 100%|██████████| 5/5 [00:03<00:00,  1.43it/s]\n",
      "Deterministic Encode: 100%|██████████| 5/5 [00:02<00:00,  1.92it/s]\n",
      "Deterministic Encode: 100%|██████████| 5/5 [00:01<00:00,  4.66it/s]\n",
      "Deterministic Encode: 100%|██████████| 5/5 [00:00<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_ID: deepseek-ai/deepseek-coder-1.3b-base\n",
      "\n",
      "2x2 matrix results:\n",
      "{'case': 'strict cpu->cpu', 'ok': True, 'elapsed_sec': 7.2, 'error': ''}\n",
      "{'case': 'strict cpu->mps', 'ok': False, 'elapsed_sec': 14.914, 'error': '_NotebookTimeoutError: Timed out after 12.0s'}\n",
      "{'case': 'best-effort cpu->cpu', 'ok': True, 'elapsed_sec': 2.517, 'error': ''}\n",
      "{'case': 'best-effort cpu->mps', 'ok': False, 'elapsed_sec': 6.763, 'error': 'RuntimeError: Decoding exceeded max_decode_tokens=32 before EOF.'}\n",
      "\n",
      "passed=2/4\n"
     ]
    }
   ],
   "source": [
    "# 2x2 determinism matrix (lightweight + hard limits)\n",
    "# axes: mode x route\n",
    "\n",
    "matrix_text = \"determinism\"\n",
    "\n",
    "cases = [\n",
    "    {\"mode\": \"strict_cpu\", \"encode_device\": \"cpu\", \"decode_device\": \"cpu\", \"label\": \"strict cpu->cpu\"},\n",
    "    {\"mode\": \"strict_cpu\", \"encode_device\": \"cpu\", \"decode_device\": \"mps\", \"label\": \"strict cpu->mps\"},\n",
    "    {\"mode\": \"gpu_best_effort\", \"encode_device\": \"cpu\", \"decode_device\": \"cpu\", \"label\": \"best-effort cpu->cpu\"},\n",
    "    {\"mode\": \"gpu_best_effort\", \"encode_device\": \"cpu\", \"decode_device\": \"mps\", \"label\": \"best-effort cpu->mps\"},\n",
    "]\n",
    "\n",
    "matrix_results = []\n",
    "for c in cases:\n",
    "    res = tiny_roundtrip_test(\n",
    "        matrix_text,\n",
    "        mode=c[\"mode\"],\n",
    "        encode_device=c[\"encode_device\"],\n",
    "        decode_device=c[\"decode_device\"],\n",
    "        max_decode_tokens=32,\n",
    "        timeout_sec=12,\n",
    "    )\n",
    "    matrix_results.append({\n",
    "        \"case\": c[\"label\"],\n",
    "        \"ok\": res.get(\"ok\", False),\n",
    "        \"elapsed_sec\": res.get(\"elapsed_sec\"),\n",
    "        \"error\": res.get(\"error\", \"\"),\n",
    "    })\n",
    "\n",
    "print(\"MODEL_ID:\", MODEL_ID)\n",
    "print(\"\\n2x2 matrix results:\")\n",
    "for r in matrix_results:\n",
    "    print(r)\n",
    "\n",
    "passed = sum(1 for r in matrix_results if r[\"ok\"])\n",
    "print(f\"\\npassed={passed}/{len(matrix_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01b91076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ok': True, 'prompt': 'determinism', 'elapsed_sec': 3.911, 'max_abs_prob_diff': 4.090224560854283e-07, 'topk': 10, 'topk_overlap': 10, 'cpu_top_tokens': ['_', '\\n', ' =', ':', '('], 'mps_top_tokens': ['_', '\\n', ' =', ':', '('], 'cpu_top_probs': [0.11621690173750832, 0.107010945866445, 0.07950255012886673, 0.07632368286833328, 0.03921738896484425], 'mps_top_probs': [0.11621649271505223, 0.10701097745782302, 0.0795025735993173, 0.07632370540033034, 0.039217400542457655]}\n"
     ]
    }
   ],
   "source": [
    "# Tiny divergence diagnostic: compare one-step next-token distribution (CPU vs MPS)\n",
    "# Safe design: single short prompt, single forward pass per device, timeout guarded.\n",
    "\n",
    "def compare_one_step_cpu_vs_mps(prompt=\"determinism\", topk=10, timeout_sec=12):\n",
    "    if tokenizer is None or model is None:\n",
    "        return {\"ok\": False, \"reason\": \"model_not_loaded\"}\n",
    "\n",
    "    if not torch.backends.mps.is_available():\n",
    "        return {\"ok\": False, \"reason\": \"mps_not_available\"}\n",
    "\n",
    "    prompt = prompt[:MAX_TEXT_CHARS]\n",
    "    token_ids = tokenizer.encode(prompt)\n",
    "    if len(token_ids) == 0:\n",
    "        token_ids = [tokenizer.bos_token_id or tokenizer.pad_token_id or 0]\n",
    "\n",
    "    cfg = DeterministicCodecConfig(\n",
    "        determinism_mode=\"strict_cpu\",\n",
    "        precision=32,\n",
    "        slots=(1 << 20),\n",
    "        use_legacy_counts=True,\n",
    "        use_kv_cache=False,\n",
    "        patch_linear=True,\n",
    "        patch_rmsnorm=True,\n",
    "        patch_attention=True,\n",
    "    )\n",
    "\n",
    "    def _get_logits_on_device(device):\n",
    "        codec = DeterministicLLMCodec(tokenizer, model, device=device, config=cfg)\n",
    "        with deterministic_runtime.deterministic_kernel_context(codec.model, codec.kernel_config):\n",
    "            logits = codec._logits_for_prefix(token_ids).detach().cpu().to(torch.float64)\n",
    "        return logits\n",
    "\n",
    "    started = time.time()\n",
    "    try:\n",
    "        logits_cpu = _run_with_timeout(_get_logits_on_device, timeout_sec, \"cpu\")\n",
    "        logits_mps = _run_with_timeout(_get_logits_on_device, timeout_sec, \"mps\")\n",
    "\n",
    "        probs_cpu = deterministic_runtime.deterministic_softmax(logits_cpu, dim=-1, mode=\"strict_cpu\").cpu().to(torch.float64)\n",
    "        probs_mps = deterministic_runtime.deterministic_softmax(logits_mps, dim=-1, mode=\"strict_cpu\").cpu().to(torch.float64)\n",
    "\n",
    "        abs_diff = (probs_cpu - probs_mps).abs()\n",
    "        max_abs_prob_diff = float(abs_diff.max().item())\n",
    "\n",
    "        top_cpu = torch.topk(probs_cpu, k=topk)\n",
    "        top_mps = torch.topk(probs_mps, k=topk)\n",
    "\n",
    "        cpu_top_ids = top_cpu.indices.tolist()\n",
    "        mps_top_ids = top_mps.indices.tolist()\n",
    "\n",
    "        overlap = len(set(cpu_top_ids).intersection(set(mps_top_ids)))\n",
    "\n",
    "        return {\n",
    "            \"ok\": True,\n",
    "            \"prompt\": prompt,\n",
    "            \"elapsed_sec\": round(time.time() - started, 3),\n",
    "            \"max_abs_prob_diff\": max_abs_prob_diff,\n",
    "            \"topk\": topk,\n",
    "            \"topk_overlap\": overlap,\n",
    "            \"cpu_top_tokens\": [tokenizer.decode([i]) for i in cpu_top_ids[:5]],\n",
    "            \"mps_top_tokens\": [tokenizer.decode([i]) for i in mps_top_ids[:5]],\n",
    "            \"cpu_top_probs\": [float(x) for x in top_cpu.values[:5]],\n",
    "            \"mps_top_probs\": [float(x) for x in top_mps.values[:5]],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"ok\": False,\n",
    "            \"error\": f\"{type(e).__name__}: {e}\",\n",
    "            \"elapsed_sec\": round(time.time() - started, 3),\n",
    "            \"traceback\": traceback.format_exc().splitlines()[-6:],\n",
    "        }\n",
    "\n",
    "\n",
    "diag_res = compare_one_step_cpu_vs_mps(prompt=\"determinism\", topk=10, timeout_sec=12)\n",
    "print(diag_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
