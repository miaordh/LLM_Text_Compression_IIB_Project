{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4a4c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"  # small-ish\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebeafa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32013, 1367, 1248, 4986]\n",
      "<｜begin▁of▁sentence｜>你好世界\n",
      "['<｜begin▁of▁sentence｜>', '你', '好', '世界']\n"
     ]
    }
   ],
   "source": [
    "# Encode some text into tokens\n",
    "text = \"你好世界\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)\n",
    "\n",
    "# Decode back into text\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(decoded)\n",
    "\n",
    "# Inspect how it split the text\n",
    "print([tokenizer.decode([t]) for t in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202975f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<｜begin▁of▁sentence｜>', 'ä½ł', 'å¥½', 'ä¸ĸçķĮ']\n",
      "vocab size: 32000\n",
      "all special tokens: ['<｜begin▁of▁sentence｜>', '<｜end▁of▁sentence｜>']\n",
      "special tokens map: {'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<｜end▁of▁sentence｜>', 'pad_token': '<｜end▁of▁sentence｜>'}\n",
      "[1367, 1248, 4986]\n",
      "['你', '好', '世界']\n"
     ]
    }
   ],
   "source": [
    "# convert ids -> token strings\n",
    "tokens = [32013, 1367, 1248, 4986]\n",
    "print(tokenizer.convert_ids_to_tokens(tokens))\n",
    "\n",
    "# vocab size and specials\n",
    "print(\"vocab size:\", tokenizer.vocab_size)\n",
    "print(\"all special tokens:\", tokenizer.all_special_tokens)\n",
    "print(\"special tokens map:\", tokenizer.special_tokens_map)\n",
    "\n",
    "# encode without adding special tokens (if you don't want BOS/EOS)\n",
    "tokens = tokenizer.encode(\"你好世界\", add_special_tokens=False)\n",
    "print(tokens)\n",
    "\n",
    "# show ID -> decoding for each id (what you already saw)\n",
    "print([tokenizer.decode([t]) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01651da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token id: 32013 token str: <｜begin▁of▁sentence｜>\n",
      "token id: 4642 token str: 轻\n",
      "token id: 4642 token str: 轻\n",
      "token id: 337 token str: 的\n",
      "token id: 848 token str: 我\n",
      "token id: 26388 token str: 走了\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 2198 token str: 正\n",
      "token id: 1410 token str: 如\n",
      "token id: 848 token str: 我\n",
      "token id: 4642 token str: 轻\n",
      "token id: 4642 token str: 轻\n",
      "token id: 337 token str: 的\n",
      "token id: 908 token str: 来\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 848 token str: 我\n",
      "token id: 4642 token str: 轻\n",
      "token id: 4642 token str: 轻\n",
      "token id: 337 token str: 的\n",
      "token id: 6716 token str: 招\n",
      "token id: 1897 token str: 手\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 1147 token str: 作\n",
      "token id: 2501 token str: 别\n",
      "token id: 2787 token str: 西\n",
      "token id: 17881 token str: 天的\n",
      "token id: 5973 token str: 云\n",
      "token id: 7817 token str: 彩\n",
      "token id: 397 token str: 。\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 1865 token str: 那\n",
      "token id: 6017 token str: 河\n",
      "token id: 1911 token str: �\n",
      "token id: 229 token str: �\n",
      "token id: 337 token str: 的\n",
      "token id: 2146 token str: 金\n",
      "token id: 23385 token str: 柳\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 502 token str: 是\n",
      "token id: 25652 token str: 夕\n",
      "token id: 4611 token str: 阳\n",
      "token id: 5374 token str: 中的\n",
      "token id: 1399 token str: 新\n",
      "token id: 15743 token str: 娘\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 7204 token str: 波\n",
      "token id: 3372 token str: 光\n",
      "token id: 10897 token str: 里的\n",
      "token id: 20550 token str: 艳\n",
      "token id: 3103 token str: 影\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 608 token str: 在\n",
      "token id: 6885 token str: 我的\n",
      "token id: 1342 token str: 心\n",
      "token id: 2641 token str: 头\n",
      "token id: 17768 token str: 荡\n",
      "token id: 3022 token str: �\n",
      "token id: 122 token str: �\n",
      "token id: 397 token str: 。\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 8857 token str: 软\n",
      "token id: 15798 token str: 泥\n",
      "token id: 5805 token str: 上的\n",
      "token id: 5025 token str: 青\n",
      "token id: 2329 token str: �\n",
      "token id: 216 token str: �\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 4324 token str: 油\n",
      "token id: 4324 token str: 油\n",
      "token id: 337 token str: 的\n",
      "token id: 608 token str: 在\n",
      "token id: 1843 token str: 水\n",
      "token id: 4795 token str: 底\n",
      "token id: 6716 token str: 招\n",
      "token id: 20148 token str: 摇\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 608 token str: 在\n",
      "token id: 5074 token str: 康\n",
      "token id: 6017 token str: 河\n",
      "token id: 337 token str: 的\n",
      "token id: 14680 token str: 柔\n",
      "token id: 7204 token str: 波\n",
      "token id: 1673 token str: 里\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 848 token str: 我\n",
      "token id: 14829 token str: 甘\n",
      "token id: 1342 token str: 心\n",
      "token id: 2077 token str: 做\n",
      "token id: 17817 token str: 一条\n",
      "token id: 1843 token str: 水\n",
      "token id: 8150 token str: 草\n",
      "token id: 2149 token str: ！\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 1865 token str: 那\n",
      "token id: 6276 token str: �\n",
      "token id: 215 token str: �\n",
      "token id: 2329 token str: �\n",
      "token id: 104 token str: �\n",
      "token id: 1153 token str: 下\n",
      "token id: 4284 token str: 的一\n",
      "token id: 6348 token str: �\n",
      "token id: 242 token str: �\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 3580 token str: 不是\n",
      "token id: 3151 token str: 清\n",
      "token id: 15370 token str: 泉\n",
      "token id: 19385 token str: ，\n",
      "token id: 502 token str: 是\n",
      "token id: 1393 token str: 天\n",
      "token id: 812 token str: 上\n",
      "token id: 2616 token str: �\n",
      "token id: 117 token str: �\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 1611 token str: �\n",
      "token id: 218 token str: �\n",
      "token id: 17162 token str: 碎\n",
      "token id: 608 token str: 在\n",
      "token id: 17066 token str: 浮\n",
      "token id: 7447 token str: �\n",
      "token id: 119 token str: �\n",
      "token id: 1651 token str: 间\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 10780 token str: 沉\n",
      "token id: 26481 token str: 淀\n",
      "token id: 1643 token str: 着\n",
      "token id: 7817 token str: 彩\n",
      "token id: 2616 token str: �\n",
      "token id: 117 token str: �\n",
      "token id: 7078 token str: 似\n",
      "token id: 337 token str: 的\n",
      "token id: 8313 token str: 梦\n",
      "token id: 397 token str: 。\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 11490 token str: 寻\n",
      "token id: 8313 token str: 梦\n",
      "token id: 2215 token str: ？\n",
      "token id: 15404 token str: 撑\n",
      "token id: 503 token str: 一\n",
      "token id: 3769 token str: 支\n",
      "token id: 1591 token str: 长\n",
      "token id: 7745 token str: �\n",
      "token id: 234 token str: �\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 2643 token str: 向\n",
      "token id: 5025 token str: 青\n",
      "token id: 8150 token str: 草\n",
      "token id: 1791 token str: 更\n",
      "token id: 5025 token str: 青\n",
      "token id: 2689 token str: 处\n",
      "token id: 12719 token str: 漫\n",
      "token id: 3083 token str: �\n",
      "token id: 107 token str: �\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 4136 token str: 满\n",
      "token id: 7438 token str: 载\n",
      "token id: 503 token str: 一\n",
      "token id: 13505 token str: 船\n",
      "token id: 4714 token str: 星\n",
      "token id: 18305 token str: 辉\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 608 token str: 在\n",
      "token id: 4714 token str: 星\n",
      "token id: 18305 token str: 辉\n",
      "token id: 18372 token str: 斑\n",
      "token id: 595 token str: �\n",
      "token id: 228 token str: �\n",
      "token id: 1673 token str: 里\n",
      "token id: 2646 token str: 放\n",
      "token id: 8028 token str: 歌\n",
      "token id: 397 token str: 。\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 1699 token str: 但\n",
      "token id: 848 token str: 我\n",
      "token id: 4867 token str: 不能\n",
      "token id: 2646 token str: 放\n",
      "token id: 8028 token str: 歌\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 28366 token str: 悄\n",
      "token id: 28366 token str: 悄\n",
      "token id: 502 token str: 是\n",
      "token id: 2501 token str: 别\n",
      "token id: 4270 token str: 离\n",
      "token id: 337 token str: 的\n",
      "token id: 1514 token str: �\n",
      "token id: 234 token str: �\n",
      "token id: 1482 token str: �\n",
      "token id: 104 token str: �\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 9020 token str: 夏\n",
      "token id: 19142 token str: 虫\n",
      "token id: 1021 token str: 也\n",
      "token id: 772 token str: 为\n",
      "token id: 848 token str: 我\n",
      "token id: 10780 token str: 沉\n",
      "token id: 12854 token str: 默\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 10780 token str: 沉\n",
      "token id: 12854 token str: 默\n",
      "token id: 502 token str: 是\n",
      "token id: 2919 token str: 今\n",
      "token id: 6046 token str: 晚\n",
      "token id: 337 token str: 的\n",
      "token id: 5074 token str: 康\n",
      "token id: 10626 token str: 桥\n",
      "token id: 2149 token str: ！\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 28366 token str: 悄\n",
      "token id: 28366 token str: 悄\n",
      "token id: 337 token str: 的\n",
      "token id: 848 token str: 我\n",
      "token id: 26388 token str: 走了\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 2198 token str: 正\n",
      "token id: 1410 token str: 如\n",
      "token id: 848 token str: 我\n",
      "token id: 28366 token str: 悄\n",
      "token id: 28366 token str: 悄\n",
      "token id: 337 token str: 的\n",
      "token id: 908 token str: 来\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 848 token str: 我\n",
      "token id: 9706 token str: 挥\n",
      "token id: 503 token str: 一\n",
      "token id: 9706 token str: 挥\n",
      "token id: 7096 token str: 衣\n",
      "token id: 29452 token str: 袖\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 570 token str: 不\n",
      "token id: 2950 token str: 带\n",
      "token id: 3366 token str: 走\n",
      "token id: 25190 token str: 一片\n",
      "token id: 5973 token str: 云\n",
      "token id: 7817 token str: 彩\n",
      "token id: 397 token str: 。\n",
      "length of tokens: 261\n"
     ]
    }
   ],
   "source": [
    "with open(\"再别康桥.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "tokens = tokenizer.encode(text)\n",
    "for t in tokens:\n",
    "    print(\"token id:\", t, \"token str:\", tokenizer.decode([t]))\n",
    "print(\"length of tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9bf6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_gemma = AutoTokenizer.from_pretrained(\"google/gemma-2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a26b5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 256000\n",
      "token id: 2 token str: <bos>\n",
      "token id: 79424 token str: 轻轻\n",
      "token id: 153698 token str: 的我\n",
      "token id: 44913 token str: 走了\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 161181 token str: 正如\n",
      "token id: 235509 token str: 我\n",
      "token id: 79424 token str: 轻轻\n",
      "token id: 235370 token str: 的\n",
      "token id: 235547 token str: 来\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235509 token str: 我\n",
      "token id: 79424 token str: 轻轻\n",
      "token id: 235370 token str: 的\n",
      "token id: 237219 token str: 招\n",
      "token id: 235616 token str: 手\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235591 token str: 作\n",
      "token id: 236273 token str: 别\n",
      "token id: 235990 token str: 西\n",
      "token id: 55881 token str: 天的\n",
      "token id: 236537 token str: 云\n",
      "token id: 236729 token str: 彩\n",
      "token id: 235362 token str: 。\n",
      "token id: 109 token str: \n",
      "\n",
      "\n",
      "token id: 235779 token str: 那\n",
      "token id: 236811 token str: 河\n",
      "token id: 241225 token str: 畔\n",
      "token id: 172765 token str: 的金\n",
      "token id: 238110 token str: 柳\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235427 token str: 是\n",
      "token id: 237957 token str: 夕\n",
      "token id: 236899 token str: 阳\n",
      "token id: 14520 token str: 中的\n",
      "token id: 208564 token str: 新娘\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 236715 token str: 波\n",
      "token id: 235973 token str: 光\n",
      "token id: 32702 token str: 里的\n",
      "token id: 239698 token str: 艳\n",
      "token id: 236006 token str: 影\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 229749 token str: 在我的\n",
      "token id: 194252 token str: 心头\n",
      "token id: 239710 token str: 荡\n",
      "token id: 242262 token str: 漾\n",
      "token id: 235362 token str: 。\n",
      "token id: 109 token str: \n",
      "\n",
      "\n",
      "token id: 237153 token str: 软\n",
      "token id: 238542 token str: 泥\n",
      "token id: 16774 token str: 上的\n",
      "token id: 236348 token str: 青\n",
      "token id: 249534 token str: 荇\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 236541 token str: 油\n",
      "token id: 236541 token str: 油\n",
      "token id: 219935 token str: 的在\n",
      "token id: 235773 token str: 水\n",
      "token id: 236507 token str: 底\n",
      "token id: 237219 token str: 招\n",
      "token id: 237996 token str: 摇\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235473 token str: 在\n",
      "token id: 236860 token str: 康\n",
      "token id: 236811 token str: 河\n",
      "token id: 235370 token str: 的\n",
      "token id: 237640 token str: 柔\n",
      "token id: 236715 token str: 波\n",
      "token id: 235792 token str: 里\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235509 token str: 我\n",
      "token id: 237599 token str: 甘\n",
      "token id: 235675 token str: 心\n",
      "token id: 235928 token str: 做\n",
      "token id: 62421 token str: 一条\n",
      "token id: 235773 token str: 水\n",
      "token id: 236726 token str: 草\n",
      "token id: 235482 token str: ！\n",
      "token id: 109 token str: \n",
      "\n",
      "\n",
      "token id: 235779 token str: 那\n",
      "token id: 243210 token str: 榆\n",
      "token id: 243665 token str: 荫\n",
      "token id: 235543 token str: 下\n",
      "token id: 13824 token str: 的一\n",
      "token id: 240701 token str: 潭\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 8835 token str: 不是\n",
      "token id: 236056 token str: 清\n",
      "token id: 237228 token str: 泉\n",
      "token id: 235365 token str: ，\n",
      "token id: 235427 token str: 是\n",
      "token id: 197162 token str: 天上\n",
      "token id: 239106 token str: 虹\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 240272 token str: 揉\n",
      "token id: 238329 token str: 碎\n",
      "token id: 235473 token str: 在\n",
      "token id: 237828 token str: 浮\n",
      "token id: 240888 token str: 藻\n",
      "token id: 235885 token str: 间\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 237473 token str: 沉\n",
      "token id: 241169 token str: 淀\n",
      "token id: 235668 token str: 着\n",
      "token id: 170684 token str: 彩虹\n",
      "token id: 95681 token str: 似的\n",
      "token id: 237278 token str: 梦\n",
      "token id: 235362 token str: 。\n",
      "token id: 109 token str: \n",
      "\n",
      "\n",
      "token id: 238166 token str: 寻\n",
      "token id: 237278 token str: 梦\n",
      "token id: 235544 token str: ？\n",
      "token id: 239694 token str: 撑\n",
      "token id: 193584 token str: 一支\n",
      "token id: 236045 token str: 长\n",
      "token id: 249697 token str: 篙\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235983 token str: 向\n",
      "token id: 236348 token str: 青\n",
      "token id: 236726 token str: 草\n",
      "token id: 235858 token str: 更\n",
      "token id: 236348 token str: 青\n",
      "token id: 236327 token str: 处\n",
      "token id: 236613 token str: 漫\n",
      "token id: 241691 token str: 溯\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 236762 token str: 满\n",
      "token id: 236640 token str: 载\n",
      "token id: 235411 token str: 一\n",
      "token id: 237166 token str: 船\n",
      "token id: 236181 token str: 星\n",
      "token id: 239392 token str: 辉\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235473 token str: 在\n",
      "token id: 236181 token str: 星\n",
      "token id: 239392 token str: 辉\n",
      "token id: 239309 token str: 斑\n",
      "token id: 245396 token str: 斓\n",
      "token id: 235792 token str: 里\n",
      "token id: 236026 token str: 放\n",
      "token id: 236481 token str: 歌\n",
      "token id: 235362 token str: 。\n",
      "token id: 109 token str: \n",
      "\n",
      "\n",
      "token id: 108698 token str: 但我\n",
      "token id: 14504 token str: 不能\n",
      "token id: 236026 token str: 放\n",
      "token id: 236481 token str: 歌\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 149757 token str: 悄悄\n",
      "token id: 235427 token str: 是\n",
      "token id: 236273 token str: 别\n",
      "token id: 236621 token str: 离\n",
      "token id: 235370 token str: 的\n",
      "token id: 241296 token str: 笙\n",
      "token id: 244435 token str: 箫\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 236535 token str: 夏\n",
      "token id: 237960 token str: 虫\n",
      "token id: 235623 token str: 也\n",
      "token id: 235640 token str: 为\n",
      "token id: 235509 token str: 我\n",
      "token id: 82018 token str: 沉默\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 82018 token str: 沉默\n",
      "token id: 235427 token str: 是\n",
      "token id: 145728 token str: 今晚\n",
      "token id: 235370 token str: 的\n",
      "token id: 236860 token str: 康\n",
      "token id: 238699 token str: 桥\n",
      "token id: 235482 token str: ！\n",
      "token id: 109 token str: \n",
      "\n",
      "\n",
      "token id: 149757 token str: 悄悄\n",
      "token id: 153698 token str: 的我\n",
      "token id: 44913 token str: 走了\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 161181 token str: 正如\n",
      "token id: 235509 token str: 我\n",
      "token id: 149757 token str: 悄悄\n",
      "token id: 235370 token str: 的\n",
      "token id: 235547 token str: 来\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235509 token str: 我\n",
      "token id: 238350 token str: 挥\n",
      "token id: 235411 token str: 一\n",
      "token id: 238350 token str: 挥\n",
      "token id: 236524 token str: 衣\n",
      "token id: 237785 token str: 袖\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235441 token str: 不\n",
      "token id: 236373 token str: 带\n",
      "token id: 236142 token str: 走\n",
      "token id: 72577 token str: 一片\n",
      "token id: 236537 token str: 云\n",
      "token id: 236729 token str: 彩\n",
      "token id: 235362 token str: 。\n",
      "length of tokens: 217\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer_gemma.encode(text)\n",
    "print(\"vocab size:\", tokenizer_gemma.vocab_size)\n",
    "for t in tokens:\n",
    "    print(\"token id:\", t, \"token str:\", tokenizer_gemma.decode([t]))\n",
    "print(\"length of tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466630c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Example with DeepSeek\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Your input so far (the context)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m inputs = \u001b[43mtokenizer\u001b[49m(text, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Example with DeepSeek\n",
    "\n",
    "# Your input so far (the context)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Run the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get logits for the next token prediction\n",
    "logits = outputs.logits[0, -1, :]  # last token's logits\n",
    "print(logits.shape)  # should be (vocab_size,)\n",
    "# print top 10 logits\n",
    "print(torch.topk(logits, 10))\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Find top probable tokens\n",
    "top_probs, top_indices = torch.topk(probs, 10)\n",
    "for i, p in zip(top_indices, top_probs):\n",
    "    print(f\"{tokenizer.decode(i)}: {p.item():.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7410421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xfc\\xd1\\xa7'\n"
     ]
    }
   ],
   "source": [
    "# test on this alphabet:\n",
    "alphabet = [\"<EOF>\", \"a\", \"e\", \"i\", \"o\", \"u\"]\n",
    "probabilities = [0.1, 0.2, 0.3, 0.15, 0.15, 0.1]\n",
    "symbol_to_id = {s: i for i, s in enumerate(alphabet)}\n",
    "message = [\"u\", \"o\", \"o\", \"e\", \"u\", \"i\", \"a\", \"<EOF>\"]\n",
    "\n",
    "import encoder\n",
    "\n",
    "enc = encoder.Encoder()\n",
    "for symbol in message:\n",
    "    id = symbol_to_id[symbol]\n",
    "    enc.encode(symbol, id, probabilities)\n",
    "print(enc.get_encoded_bytes())  # print as integer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48948b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model counts (first few): [6554, 13107, 19661, 9830, 9830, 6553] sum: 65535\n",
      "cum_desc[0] (total): 65535\n",
      "Encoded bytes (hex): 5400\n",
      "Encoded bitstring (bytes*8 bits shown): 0101010000000000\n",
      "After encoding: encoder L= 0 R= 32768 bits_waiting= 0\n",
      "Decoder primed: L= 0 D= 21504 R= 65536\n",
      "decoded step 0: idx=3, symbol=o\n",
      "decoded step 1: idx=2, symbol=i\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Decoder invalid target=-15971 at step 2 (L=58979, D=43008, R=19661)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 140\u001b[39m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDecoder: invalid R=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m at step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m0\u001b[39m <= target < R):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDecoder invalid target=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m at step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (L=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoder_dec.L\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, D=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoder_dec.D\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, R=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# textbook mapping to integer count\u001b[39;00m\n\u001b[32m    143\u001b[39m count = ((target + \u001b[32m1\u001b[39m) * total - \u001b[32m1\u001b[39m) // R\n",
      "\u001b[31mRuntimeError\u001b[39m: Decoder invalid target=-15971 at step 2 (L=58979, D=43008, R=19661)"
     ]
    }
   ],
   "source": [
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from arithmetic_coding import Coder\n",
    "from bitReadWrite import BitReader, BitWriter\n",
    "\n",
    "# Jupyter test cell: end-to-end encode/decode using your Coder / Encoder / Decoder classes.\n",
    "# Paste this cell below where you have defined or imported Coder, Encoder, Decoder.\n",
    "\n",
    "from typing import List\n",
    "import bisect\n",
    "\n",
    "# -------------------------\n",
    "# Small helpers (copy into cell so test is self-contained)\n",
    "# -------------------------\n",
    "def probs_to_counts_largest_remainder(probs: List[float], slots: int) -> List[int]:\n",
    "    \"\"\"Deterministic mapping of floats -> integer counts summing to exactly 'slots'.\"\"\"\n",
    "    if len(probs) == 0:\n",
    "        raise ValueError(\"Empty probability list\")\n",
    "    raw = [float(p) for p in probs]\n",
    "    s = sum(raw)\n",
    "    if s <= 0:\n",
    "        raise ValueError(\"Probabilities must sum to > 0\")\n",
    "    scaled = [r * slots / s for r in raw]\n",
    "    floors = [int(x) for x in scaled]\n",
    "    remainder = slots - sum(floors)\n",
    "    fracs = sorted(((i, scaled[i] - floors[i]) for i in range(len(probs))),\n",
    "                   key=lambda x: x[1], reverse=True)\n",
    "    i = 0\n",
    "    while remainder > 0 and i < len(fracs):\n",
    "        floors[fracs[i][0]] += 1\n",
    "        remainder -= 1\n",
    "        i += 1\n",
    "    # safety: if a nonzero prob got zero count we must increase slots/precision\n",
    "    for p, c in zip(raw, floors):\n",
    "        if p > 0.0 and c == 0:\n",
    "            raise ValueError(\"Precision too low: a nonzero probability mapped to 0 count\")\n",
    "    return floors\n",
    "\n",
    "def counts_to_cum_desc(counts: List[int]) -> List[int]:\n",
    "    \"\"\"Convert counts -> descending cumulative array used by CACM: cum_desc[0]=total, cum_desc[-1]=0.\"\"\"\n",
    "    total = sum(counts)\n",
    "    cum = [total]\n",
    "    s = 0\n",
    "    for c in counts:\n",
    "        s += c\n",
    "        cum.append(total - s)\n",
    "    return cum  # length n+1, cum[0]=total, cum[n]=0\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Test config (your example)\n",
    "# -------------------------\n",
    "alphabet = ['a', 'e', 'i', 'o', 'u', '<EOF>']\n",
    "probs = [0.1, 0.2, 0.3, 0.15, 0.15, 0.1]\n",
    "message = [\"u\", \"o\", \"o\", \"e\", \"u\", \"i\", \"a\", \"<EOF>\"]\n",
    "\n",
    "# -------------------------\n",
    "# Ensure Coder / Encoder / Decoder exist (user's classes)\n",
    "# -------------------------\n",
    "if 'Coder' not in globals():\n",
    "    raise RuntimeError(\"Coder class not found in notebook scope. Define/import Coder before running this cell.\")\n",
    "if 'Encoder' not in globals():\n",
    "    raise RuntimeError(\"Encoder class not found in notebook scope. Define/import Encoder before running this cell.\")\n",
    "if 'Decoder' not in globals():\n",
    "    raise RuntimeError(\"Decoder class not found in notebook scope. Define/import Decoder before running this cell.\")\n",
    "\n",
    "# -------------------------\n",
    "# Build model counts and cumulative (descending) expected by textbook wrappers\n",
    "# -------------------------\n",
    "# instantiate a temporary Coder to choose slots consistent with its precision\n",
    "temp_coder = globals()['Coder'](b=16)   # choose b=16 for compactness; increase to 32 to match paper if desired\n",
    "# slots: choose temp_coder.tb ( = 2^b - 1 ) so total fits inside coder precision\n",
    "slots = temp_coder.tb\n",
    "counts = probs_to_counts_largest_remainder(probs, slots)\n",
    "cum_desc = counts_to_cum_desc(counts)\n",
    "\n",
    "print(\"Model counts (first few):\", counts[:6], \"sum:\", sum(counts))\n",
    "print(\"cum_desc[0] (total):\", cum_desc[0])\n",
    "\n",
    "# -------------------------\n",
    "# ENCODING (use Coder.storeRegion directly to avoid wrapper convention mismatch)\n",
    "# -------------------------\n",
    "coder_enc = globals()['Coder'](b=16)\n",
    "bw = BitWriter()\n",
    "\n",
    "# prime the encoder (your Coder method is start_encode)\n",
    "if hasattr(coder_enc, 'start_encode'):\n",
    "    coder_enc.start_encode(bw)\n",
    "else:\n",
    "    raise RuntimeError(\"Coder missing start_encode; adapt call to your API.\")\n",
    "\n",
    "sym_to_idx = {s: i for i, s in enumerate(alphabet)}\n",
    "\n",
    "# perform symbol-by-symbol encoding using textbook descending cum_desc convention\n",
    "for sym in message:\n",
    "    idx = sym_to_idx[sym]\n",
    "    l = cum_desc[idx + 1]\n",
    "    h = cum_desc[idx]\n",
    "    # call the exact method you defined earlier\n",
    "    coder_enc.storeRegion(l, h)\n",
    "\n",
    "# finalize encoder and flush bit-writer\n",
    "coder_enc.finish_encode()\n",
    "bw.flush(padbit=0)   # your BitWriter implementation expects padbit param\n",
    "encoded_bytes = bw.getvalue()\n",
    "print(\"Encoded bytes (hex):\", encoded_bytes.hex())\n",
    "print(\"Encoded bitstring (bytes*8 bits shown):\", ''.join(f'{b:08b}' for b in encoded_bytes))\n",
    "\n",
    "# diagnostic: print coder_enc internal state after encoding finishes\n",
    "print(\"After encoding: encoder L=\", coder_enc.L, \"R=\", coder_enc.R, \"bits_waiting=\", coder_enc.bits_waiting)\n",
    "# prime a new decoder and print its initial state\n",
    "coder_dec = globals()['Coder'](b=16)\n",
    "coder_dec.start_decode(BitReader(encoded_bytes))\n",
    "print(\"Decoder primed: L=\", coder_dec.L, \"D=\", coder_dec.D, \"R=\", coder_dec.R)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# DECODING (direct Coder usage — uses attributes L,R,D and loadRegion)\n",
    "# -------------------------\n",
    "coder_dec = globals()['Coder'](b=16)\n",
    "br = BitReader(encoded_bytes)\n",
    "\n",
    "# prime decoder\n",
    "if hasattr(coder_dec, 'start_decode'):\n",
    "    coder_dec.start_decode(br)\n",
    "else:\n",
    "    raise RuntimeError(\"Coder missing start_decode; adapt call to your API.\")\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(message)):\n",
    "    # read the current integer range and target using the Coder attributes\n",
    "    R = coder_dec.R\n",
    "    target = coder_dec.D - coder_dec.L\n",
    "    total = cum_desc[0]\n",
    "\n",
    "    # defensive checks\n",
    "    if R <= 0:\n",
    "        raise RuntimeError(f\"Decoder: invalid R={R} at step {i}\")\n",
    "    if not (0 <= target < R):\n",
    "        raise RuntimeError(f\"Decoder invalid target={target} at step {i} (L={coder_dec.L}, D={coder_dec.D}, R={R})\")\n",
    "\n",
    "    # textbook mapping to integer count\n",
    "    count = ((target + 1) * total - 1) // R\n",
    "\n",
    "    # find symbol s with cum_desc[s+1] <= count < cum_desc[s]\n",
    "    s_found = None\n",
    "    for s in range(len(cum_desc) - 1):\n",
    "        if cum_desc[s + 1] <= count < cum_desc[s]:\n",
    "            s_found = s\n",
    "            break\n",
    "    if s_found is None:\n",
    "        s_found = len(cum_desc) - 2\n",
    "\n",
    "    # update coder state by loading the symbol's region\n",
    "    l = cum_desc[s_found + 1]\n",
    "    h = cum_desc[s_found]\n",
    "    coder_dec.loadRegion(l, h)\n",
    "\n",
    "    decoded.append(alphabet[s_found])\n",
    "    print(f\"decoded step {i}: idx={s_found}, symbol={alphabet[s_found]}\")\n",
    "\n",
    "print(\"Decoded message:\", decoded)\n",
    "print(\"Round-trip OK?\", decoded == message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f3981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model counts: [429496730, 858993459, 1288490189, 644245094, 644245094, 429496729] total: 4294967295\n",
      "Encoded bytes (hex): dab872\n",
      "Decoded: ['u', 'o', 'o', 'e', 'u', 'i', 'a', '<EOF>']\n",
      "Round-trip OK? True\n"
     ]
    }
   ],
   "source": [
    "from utils import probs_to_counts_largest_remainder, counts_to_cum_desc\n",
    "from arithmetic_coding import Coder\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from bitReadWrite import BitReader, BitWriter\n",
    "\n",
    "alphabet = ['a', 'e', 'i', 'o', 'u', '<EOF>']\n",
    "probs = [0.1, 0.2, 0.3, 0.15, 0.15, 0.1]\n",
    "message = [\"u\", \"o\", \"o\", \"e\", \"u\", \"i\", \"a\", \"<EOF>\"]\n",
    "\n",
    "# choose precision b=32 for quickness\n",
    "coder_temp = Coder(b=32)\n",
    "slots = coder_temp.tb  # = 2^b - 1\n",
    "counts = probs_to_counts_largest_remainder(probs, slots)\n",
    "cum_desc = counts_to_cum_desc(counts)\n",
    "\n",
    "print(\"Model counts:\", counts, \"total:\", cum_desc[0])\n",
    "\n",
    "# encode\n",
    "cw = BitWriter()\n",
    "coder_enc = Coder(b=32)\n",
    "enc = Encoder(coder_enc, cw)\n",
    "sym_to_idx = {s: i for i, s in enumerate(alphabet)}\n",
    "for sym in message:\n",
    "    idx = sym_to_idx[sym]\n",
    "    enc.encode_symbol(idx, cum_desc)\n",
    "enc.finish()\n",
    "cw.flush(padbit=0)\n",
    "encoded = cw.getvalue()\n",
    "print(\"Encoded bytes (hex):\", encoded.hex())\n",
    "\n",
    "# decode\n",
    "br = BitReader(encoded)\n",
    "coder_dec = Coder(b=32)\n",
    "dec = Decoder(coder_dec, br)\n",
    "decoded = []\n",
    "for _ in range(len(message)):\n",
    "    idx = dec.decode_symbol(cum_desc)\n",
    "    decoded.append(alphabet[idx])\n",
    "print(\"Decoded:\", decoded)\n",
    "print(\"Round-trip OK?\", decoded == message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ddf37c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding tokens. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  7.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded bytes (hex): 6185d1ad8e6f651646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llm_codec import LLM_Encode\n",
    "text = \"This is a test of LLM-based encoding.\"\n",
    "llm_encoder = LLM_Encode(tokenizer=tokenizer, model=model, precision=128)\n",
    "encoded_bytes, text_length = llm_encoder.encode(text)\n",
    "print(\"Encoded bytes (hex):\", encoded_bytes.hex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb9c8cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding tokens. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text matches original? False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llm_codec import LLM_Decode\n",
    "llm_decoder = LLM_Decode(tokenizer=tokenizer, model=model, precision=128)\n",
    "decoded_text = llm_decoder.decode(encoded_bytes, text_length)\n",
    "print(\"Decoded text matches original?\", decoded_text == text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
