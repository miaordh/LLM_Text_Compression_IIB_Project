{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4a4c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"  # small-ish\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebeafa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32013, 1367, 1248, 4986]\n",
      "<｜begin▁of▁sentence｜>你好世界\n",
      "['<｜begin▁of▁sentence｜>', '你', '好', '世界']\n"
     ]
    }
   ],
   "source": [
    "# Encode some text into tokens\n",
    "text = \"你好世界\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)\n",
    "\n",
    "# Decode back into text\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(decoded)\n",
    "\n",
    "# Inspect how it split the text\n",
    "print([tokenizer.decode([t]) for t in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202975f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<｜begin▁of▁sentence｜>', 'ä½ł', 'å¥½', 'ä¸ĸçķĮ']\n",
      "vocab size: 32000\n",
      "all special tokens: ['<｜begin▁of▁sentence｜>', '<｜end▁of▁sentence｜>']\n",
      "special tokens map: {'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<｜end▁of▁sentence｜>', 'pad_token': '<｜end▁of▁sentence｜>'}\n",
      "[1367, 1248, 4986]\n",
      "['你', '好', '世界']\n"
     ]
    }
   ],
   "source": [
    "# convert ids -> token strings\n",
    "tokens = [32013, 1367, 1248, 4986]\n",
    "print(tokenizer.convert_ids_to_tokens(tokens))\n",
    "\n",
    "# vocab size and specials\n",
    "print(\"vocab size:\", tokenizer.vocab_size)\n",
    "print(\"all special tokens:\", tokenizer.all_special_tokens)\n",
    "print(\"special tokens map:\", tokenizer.special_tokens_map)\n",
    "\n",
    "# encode without adding special tokens (if you don't want BOS/EOS)\n",
    "tokens = tokenizer.encode(\"你好世界\", add_special_tokens=False)\n",
    "print(tokens)\n",
    "\n",
    "# show ID -> decoding for each id (what you already saw)\n",
    "print([tokenizer.decode([t]) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01651da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token id: 32013 token str: <｜begin▁of▁sentence｜>\n",
      "token id: 4642 token str: 轻\n",
      "token id: 4642 token str: 轻\n",
      "token id: 337 token str: 的\n",
      "token id: 848 token str: 我\n",
      "token id: 26388 token str: 走了\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 2198 token str: 正\n",
      "token id: 1410 token str: 如\n",
      "token id: 848 token str: 我\n",
      "token id: 4642 token str: 轻\n",
      "token id: 4642 token str: 轻\n",
      "token id: 337 token str: 的\n",
      "token id: 908 token str: 来\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 848 token str: 我\n",
      "token id: 4642 token str: 轻\n",
      "token id: 4642 token str: 轻\n",
      "token id: 337 token str: 的\n",
      "token id: 6716 token str: 招\n",
      "token id: 1897 token str: 手\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 1147 token str: 作\n",
      "token id: 2501 token str: 别\n",
      "token id: 2787 token str: 西\n",
      "token id: 17881 token str: 天的\n",
      "token id: 5973 token str: 云\n",
      "token id: 7817 token str: 彩\n",
      "token id: 397 token str: 。\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 1865 token str: 那\n",
      "token id: 6017 token str: 河\n",
      "token id: 1911 token str: �\n",
      "token id: 229 token str: �\n",
      "token id: 337 token str: 的\n",
      "token id: 2146 token str: 金\n",
      "token id: 23385 token str: 柳\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 502 token str: 是\n",
      "token id: 25652 token str: 夕\n",
      "token id: 4611 token str: 阳\n",
      "token id: 5374 token str: 中的\n",
      "token id: 1399 token str: 新\n",
      "token id: 15743 token str: 娘\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 7204 token str: 波\n",
      "token id: 3372 token str: 光\n",
      "token id: 10897 token str: 里的\n",
      "token id: 20550 token str: 艳\n",
      "token id: 3103 token str: 影\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 608 token str: 在\n",
      "token id: 6885 token str: 我的\n",
      "token id: 1342 token str: 心\n",
      "token id: 2641 token str: 头\n",
      "token id: 17768 token str: 荡\n",
      "token id: 3022 token str: �\n",
      "token id: 122 token str: �\n",
      "token id: 397 token str: 。\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 8857 token str: 软\n",
      "token id: 15798 token str: 泥\n",
      "token id: 5805 token str: 上的\n",
      "token id: 5025 token str: 青\n",
      "token id: 2329 token str: �\n",
      "token id: 216 token str: �\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 4324 token str: 油\n",
      "token id: 4324 token str: 油\n",
      "token id: 337 token str: 的\n",
      "token id: 608 token str: 在\n",
      "token id: 1843 token str: 水\n",
      "token id: 4795 token str: 底\n",
      "token id: 6716 token str: 招\n",
      "token id: 20148 token str: 摇\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 608 token str: 在\n",
      "token id: 5074 token str: 康\n",
      "token id: 6017 token str: 河\n",
      "token id: 337 token str: 的\n",
      "token id: 14680 token str: 柔\n",
      "token id: 7204 token str: 波\n",
      "token id: 1673 token str: 里\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 848 token str: 我\n",
      "token id: 14829 token str: 甘\n",
      "token id: 1342 token str: 心\n",
      "token id: 2077 token str: 做\n",
      "token id: 17817 token str: 一条\n",
      "token id: 1843 token str: 水\n",
      "token id: 8150 token str: 草\n",
      "token id: 2149 token str: ！\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 1865 token str: 那\n",
      "token id: 6276 token str: �\n",
      "token id: 215 token str: �\n",
      "token id: 2329 token str: �\n",
      "token id: 104 token str: �\n",
      "token id: 1153 token str: 下\n",
      "token id: 4284 token str: 的一\n",
      "token id: 6348 token str: �\n",
      "token id: 242 token str: �\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 3580 token str: 不是\n",
      "token id: 3151 token str: 清\n",
      "token id: 15370 token str: 泉\n",
      "token id: 19385 token str: ，\n",
      "token id: 502 token str: 是\n",
      "token id: 1393 token str: 天\n",
      "token id: 812 token str: 上\n",
      "token id: 2616 token str: �\n",
      "token id: 117 token str: �\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 1611 token str: �\n",
      "token id: 218 token str: �\n",
      "token id: 17162 token str: 碎\n",
      "token id: 608 token str: 在\n",
      "token id: 17066 token str: 浮\n",
      "token id: 7447 token str: �\n",
      "token id: 119 token str: �\n",
      "token id: 1651 token str: 间\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 10780 token str: 沉\n",
      "token id: 26481 token str: 淀\n",
      "token id: 1643 token str: 着\n",
      "token id: 7817 token str: 彩\n",
      "token id: 2616 token str: �\n",
      "token id: 117 token str: �\n",
      "token id: 7078 token str: 似\n",
      "token id: 337 token str: 的\n",
      "token id: 8313 token str: 梦\n",
      "token id: 397 token str: 。\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 11490 token str: 寻\n",
      "token id: 8313 token str: 梦\n",
      "token id: 2215 token str: ？\n",
      "token id: 15404 token str: 撑\n",
      "token id: 503 token str: 一\n",
      "token id: 3769 token str: 支\n",
      "token id: 1591 token str: 长\n",
      "token id: 7745 token str: �\n",
      "token id: 234 token str: �\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 2643 token str: 向\n",
      "token id: 5025 token str: 青\n",
      "token id: 8150 token str: 草\n",
      "token id: 1791 token str: 更\n",
      "token id: 5025 token str: 青\n",
      "token id: 2689 token str: 处\n",
      "token id: 12719 token str: 漫\n",
      "token id: 3083 token str: �\n",
      "token id: 107 token str: �\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 4136 token str: 满\n",
      "token id: 7438 token str: 载\n",
      "token id: 503 token str: 一\n",
      "token id: 13505 token str: 船\n",
      "token id: 4714 token str: 星\n",
      "token id: 18305 token str: 辉\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 608 token str: 在\n",
      "token id: 4714 token str: 星\n",
      "token id: 18305 token str: 辉\n",
      "token id: 18372 token str: 斑\n",
      "token id: 595 token str: �\n",
      "token id: 228 token str: �\n",
      "token id: 1673 token str: 里\n",
      "token id: 2646 token str: 放\n",
      "token id: 8028 token str: 歌\n",
      "token id: 397 token str: 。\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 1699 token str: 但\n",
      "token id: 848 token str: 我\n",
      "token id: 4867 token str: 不能\n",
      "token id: 2646 token str: 放\n",
      "token id: 8028 token str: 歌\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 28366 token str: 悄\n",
      "token id: 28366 token str: 悄\n",
      "token id: 502 token str: 是\n",
      "token id: 2501 token str: 别\n",
      "token id: 4270 token str: 离\n",
      "token id: 337 token str: 的\n",
      "token id: 1514 token str: �\n",
      "token id: 234 token str: �\n",
      "token id: 1482 token str: �\n",
      "token id: 104 token str: �\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 9020 token str: 夏\n",
      "token id: 19142 token str: 虫\n",
      "token id: 1021 token str: 也\n",
      "token id: 772 token str: 为\n",
      "token id: 848 token str: 我\n",
      "token id: 10780 token str: 沉\n",
      "token id: 12854 token str: 默\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 10780 token str: 沉\n",
      "token id: 12854 token str: 默\n",
      "token id: 502 token str: 是\n",
      "token id: 2919 token str: 今\n",
      "token id: 6046 token str: 晚\n",
      "token id: 337 token str: 的\n",
      "token id: 5074 token str: 康\n",
      "token id: 10626 token str: 桥\n",
      "token id: 2149 token str: ！\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 28366 token str: 悄\n",
      "token id: 28366 token str: 悄\n",
      "token id: 337 token str: 的\n",
      "token id: 848 token str: 我\n",
      "token id: 26388 token str: 走了\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 2198 token str: 正\n",
      "token id: 1410 token str: 如\n",
      "token id: 848 token str: 我\n",
      "token id: 28366 token str: 悄\n",
      "token id: 28366 token str: 悄\n",
      "token id: 337 token str: 的\n",
      "token id: 908 token str: 来\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 848 token str: 我\n",
      "token id: 9706 token str: 挥\n",
      "token id: 503 token str: 一\n",
      "token id: 9706 token str: 挥\n",
      "token id: 7096 token str: 衣\n",
      "token id: 29452 token str: 袖\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 570 token str: 不\n",
      "token id: 2950 token str: 带\n",
      "token id: 3366 token str: 走\n",
      "token id: 25190 token str: 一片\n",
      "token id: 5973 token str: 云\n",
      "token id: 7817 token str: 彩\n",
      "token id: 397 token str: 。\n",
      "length of tokens: 261\n"
     ]
    }
   ],
   "source": [
    "with open(\"再别康桥.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "tokens = tokenizer.encode(text)\n",
    "for t in tokens:\n",
    "    print(\"token id:\", t, \"token str:\", tokenizer.decode([t]))\n",
    "print(\"length of tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9bf6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_gemma = AutoTokenizer.from_pretrained(\"google/gemma-2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a26b5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 256000\n",
      "token id: 2 token str: <bos>\n",
      "token id: 79424 token str: 轻轻\n",
      "token id: 153698 token str: 的我\n",
      "token id: 44913 token str: 走了\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 161181 token str: 正如\n",
      "token id: 235509 token str: 我\n",
      "token id: 79424 token str: 轻轻\n",
      "token id: 235370 token str: 的\n",
      "token id: 235547 token str: 来\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235509 token str: 我\n",
      "token id: 79424 token str: 轻轻\n",
      "token id: 235370 token str: 的\n",
      "token id: 237219 token str: 招\n",
      "token id: 235616 token str: 手\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235591 token str: 作\n",
      "token id: 236273 token str: 别\n",
      "token id: 235990 token str: 西\n",
      "token id: 55881 token str: 天的\n",
      "token id: 236537 token str: 云\n",
      "token id: 236729 token str: 彩\n",
      "token id: 235362 token str: 。\n",
      "token id: 109 token str: \n",
      "\n",
      "\n",
      "token id: 235779 token str: 那\n",
      "token id: 236811 token str: 河\n",
      "token id: 241225 token str: 畔\n",
      "token id: 172765 token str: 的金\n",
      "token id: 238110 token str: 柳\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235427 token str: 是\n",
      "token id: 237957 token str: 夕\n",
      "token id: 236899 token str: 阳\n",
      "token id: 14520 token str: 中的\n",
      "token id: 208564 token str: 新娘\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 236715 token str: 波\n",
      "token id: 235973 token str: 光\n",
      "token id: 32702 token str: 里的\n",
      "token id: 239698 token str: 艳\n",
      "token id: 236006 token str: 影\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 229749 token str: 在我的\n",
      "token id: 194252 token str: 心头\n",
      "token id: 239710 token str: 荡\n",
      "token id: 242262 token str: 漾\n",
      "token id: 235362 token str: 。\n",
      "token id: 109 token str: \n",
      "\n",
      "\n",
      "token id: 237153 token str: 软\n",
      "token id: 238542 token str: 泥\n",
      "token id: 16774 token str: 上的\n",
      "token id: 236348 token str: 青\n",
      "token id: 249534 token str: 荇\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 236541 token str: 油\n",
      "token id: 236541 token str: 油\n",
      "token id: 219935 token str: 的在\n",
      "token id: 235773 token str: 水\n",
      "token id: 236507 token str: 底\n",
      "token id: 237219 token str: 招\n",
      "token id: 237996 token str: 摇\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235473 token str: 在\n",
      "token id: 236860 token str: 康\n",
      "token id: 236811 token str: 河\n",
      "token id: 235370 token str: 的\n",
      "token id: 237640 token str: 柔\n",
      "token id: 236715 token str: 波\n",
      "token id: 235792 token str: 里\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235509 token str: 我\n",
      "token id: 237599 token str: 甘\n",
      "token id: 235675 token str: 心\n",
      "token id: 235928 token str: 做\n",
      "token id: 62421 token str: 一条\n",
      "token id: 235773 token str: 水\n",
      "token id: 236726 token str: 草\n",
      "token id: 235482 token str: ！\n",
      "token id: 109 token str: \n",
      "\n",
      "\n",
      "token id: 235779 token str: 那\n",
      "token id: 243210 token str: 榆\n",
      "token id: 243665 token str: 荫\n",
      "token id: 235543 token str: 下\n",
      "token id: 13824 token str: 的一\n",
      "token id: 240701 token str: 潭\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 8835 token str: 不是\n",
      "token id: 236056 token str: 清\n",
      "token id: 237228 token str: 泉\n",
      "token id: 235365 token str: ，\n",
      "token id: 235427 token str: 是\n",
      "token id: 197162 token str: 天上\n",
      "token id: 239106 token str: 虹\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 240272 token str: 揉\n",
      "token id: 238329 token str: 碎\n",
      "token id: 235473 token str: 在\n",
      "token id: 237828 token str: 浮\n",
      "token id: 240888 token str: 藻\n",
      "token id: 235885 token str: 间\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 237473 token str: 沉\n",
      "token id: 241169 token str: 淀\n",
      "token id: 235668 token str: 着\n",
      "token id: 170684 token str: 彩虹\n",
      "token id: 95681 token str: 似的\n",
      "token id: 237278 token str: 梦\n",
      "token id: 235362 token str: 。\n",
      "token id: 109 token str: \n",
      "\n",
      "\n",
      "token id: 238166 token str: 寻\n",
      "token id: 237278 token str: 梦\n",
      "token id: 235544 token str: ？\n",
      "token id: 239694 token str: 撑\n",
      "token id: 193584 token str: 一支\n",
      "token id: 236045 token str: 长\n",
      "token id: 249697 token str: 篙\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235983 token str: 向\n",
      "token id: 236348 token str: 青\n",
      "token id: 236726 token str: 草\n",
      "token id: 235858 token str: 更\n",
      "token id: 236348 token str: 青\n",
      "token id: 236327 token str: 处\n",
      "token id: 236613 token str: 漫\n",
      "token id: 241691 token str: 溯\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 236762 token str: 满\n",
      "token id: 236640 token str: 载\n",
      "token id: 235411 token str: 一\n",
      "token id: 237166 token str: 船\n",
      "token id: 236181 token str: 星\n",
      "token id: 239392 token str: 辉\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235473 token str: 在\n",
      "token id: 236181 token str: 星\n",
      "token id: 239392 token str: 辉\n",
      "token id: 239309 token str: 斑\n",
      "token id: 245396 token str: 斓\n",
      "token id: 235792 token str: 里\n",
      "token id: 236026 token str: 放\n",
      "token id: 236481 token str: 歌\n",
      "token id: 235362 token str: 。\n",
      "token id: 109 token str: \n",
      "\n",
      "\n",
      "token id: 108698 token str: 但我\n",
      "token id: 14504 token str: 不能\n",
      "token id: 236026 token str: 放\n",
      "token id: 236481 token str: 歌\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 149757 token str: 悄悄\n",
      "token id: 235427 token str: 是\n",
      "token id: 236273 token str: 别\n",
      "token id: 236621 token str: 离\n",
      "token id: 235370 token str: 的\n",
      "token id: 241296 token str: 笙\n",
      "token id: 244435 token str: 箫\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 236535 token str: 夏\n",
      "token id: 237960 token str: 虫\n",
      "token id: 235623 token str: 也\n",
      "token id: 235640 token str: 为\n",
      "token id: 235509 token str: 我\n",
      "token id: 82018 token str: 沉默\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 82018 token str: 沉默\n",
      "token id: 235427 token str: 是\n",
      "token id: 145728 token str: 今晚\n",
      "token id: 235370 token str: 的\n",
      "token id: 236860 token str: 康\n",
      "token id: 238699 token str: 桥\n",
      "token id: 235482 token str: ！\n",
      "token id: 109 token str: \n",
      "\n",
      "\n",
      "token id: 149757 token str: 悄悄\n",
      "token id: 153698 token str: 的我\n",
      "token id: 44913 token str: 走了\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 161181 token str: 正如\n",
      "token id: 235509 token str: 我\n",
      "token id: 149757 token str: 悄悄\n",
      "token id: 235370 token str: 的\n",
      "token id: 235547 token str: 来\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235509 token str: 我\n",
      "token id: 238350 token str: 挥\n",
      "token id: 235411 token str: 一\n",
      "token id: 238350 token str: 挥\n",
      "token id: 236524 token str: 衣\n",
      "token id: 237785 token str: 袖\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235441 token str: 不\n",
      "token id: 236373 token str: 带\n",
      "token id: 236142 token str: 走\n",
      "token id: 72577 token str: 一片\n",
      "token id: 236537 token str: 云\n",
      "token id: 236729 token str: 彩\n",
      "token id: 235362 token str: 。\n",
      "length of tokens: 217\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer_gemma.encode(text)\n",
    "print(\"vocab size:\", tokenizer_gemma.vocab_size)\n",
    "for t in tokens:\n",
    "    print(\"token id:\", t, \"token str:\", tokenizer_gemma.decode([t]))\n",
    "print(\"length of tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "466630c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32256])\n",
      "torch.return_types.topk(\n",
      "values=tensor([ -5.1992,  -9.2840, -12.1456, -12.1604, -12.3317, -12.6403, -12.7734,\n",
      "        -12.8906, -13.4317, -13.4851]),\n",
      "indices=tensor([  185, 32014,    63,   207,    58,   971,  1183,    59,  4191,   397]))\n",
      "\n",
      ": 0.97643\n",
      "<｜end▁of▁sentence｜>: 0.01643\n",
      "`: 0.00094\n",
      " : 0.00093\n",
      "[: 0.00078\n",
      "”: 0.00057\n",
      "',: 0.00050\n",
      "\\: 0.00045\n",
      "'': 0.00026\n",
      "。: 0.00025\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Example with DeepSeek\n",
    "\n",
    "# Your input so far (the context)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Run the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get logits for the next token prediction\n",
    "logits = outputs.logits[0, -1, :]  # last token's logits\n",
    "print(logits.shape)  # should be (vocab_size,)\n",
    "# print top 10 logits\n",
    "print(torch.topk(logits, 10))\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Find top probable tokens\n",
    "top_probs, top_indices = torch.topk(probs, 10)\n",
    "for i, p in zip(top_indices, top_probs):\n",
    "    print(f\"{tokenizer.decode(i)}: {p.item():.5f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
