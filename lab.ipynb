{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a4c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"  # small-ish\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebeafa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32013, 1367, 1248, 4986]\n",
      "<｜begin▁of▁sentence｜>你好世界\n",
      "['<｜begin▁of▁sentence｜>', '你', '好', '世界']\n"
     ]
    }
   ],
   "source": [
    "# Encode some text into tokens\n",
    "text = \"你好世界\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)\n",
    "\n",
    "# Decode back into text\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(decoded)\n",
    "\n",
    "# Inspect how it split the text\n",
    "print([tokenizer.decode([t]) for t in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "202975f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<｜begin▁of▁sentence｜>', 'ä½ł', 'å¥½', 'ä¸ĸçķĮ']\n",
      "vocab size: 32000\n",
      "all special tokens: ['<｜begin▁of▁sentence｜>', '<｜end▁of▁sentence｜>']\n",
      "special tokens map: {'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<｜end▁of▁sentence｜>', 'pad_token': '<｜end▁of▁sentence｜>'}\n",
      "[1367, 1248, 4986]\n",
      "['你', '好', '世界']\n"
     ]
    }
   ],
   "source": [
    "# convert ids -> token strings\n",
    "tokens = [32013, 1367, 1248, 4986]\n",
    "print(tokenizer.convert_ids_to_tokens(tokens))\n",
    "\n",
    "# vocab size and specials\n",
    "print(\"vocab size:\", tokenizer.vocab_size)\n",
    "print(\"all special tokens:\", tokenizer.all_special_tokens)\n",
    "print(\"special tokens map:\", tokenizer.special_tokens_map)\n",
    "\n",
    "# encode without adding special tokens (if you don't want BOS/EOS)\n",
    "tokens = tokenizer.encode(\"你好世界\", add_special_tokens=False)\n",
    "print(tokens)\n",
    "\n",
    "# show ID -> decoding for each id (what you already saw)\n",
    "print([tokenizer.decode([t]) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01651da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轻轻的我走了，\n",
      "正如我轻轻的来；\n",
      "我轻轻的招手，\n",
      "作别西天的云彩。\n",
      "\n",
      "那河畔的金柳，\n",
      "是夕阳中的新娘；\n",
      "波光里的艳影，\n",
      "在我的心头荡漾。\n",
      "\n",
      "软泥上的青荇，\n",
      "油油的在水底招摇；\n",
      "在康河的柔波里，\n",
      "我甘心做一条水草！\n",
      "\n",
      "那榆荫下的一潭，\n",
      "不是清泉，是天上虹；\n",
      "揉碎在浮藻间，\n",
      "沉淀着彩虹似的梦。\n",
      "\n",
      "寻梦？撑一支长篙，\n",
      "向青草更青处漫溯；\n",
      "满载一船星辉，\n",
      "在星辉斑斓里放歌。\n",
      "\n",
      "但我不能放歌，\n",
      "悄悄是别离的笙箫；\n",
      "夏虫也为我沉默，\n",
      "沉默是今晚的康桥！\n",
      "\n",
      "悄悄的我走了，\n",
      "正如我悄悄的来；\n",
      "我挥一挥衣袖，\n",
      "不带走一片云彩。\n",
      "['<｜begin▁of▁sentence｜>', '轻', '轻', '的', '我', '走了', '，', '\\n', '正', '如', '我', '轻', '轻', '的', '来', '；', '\\n', '我', '轻', '轻', '的', '招', '手', '，', '\\n', '作', '别', '西', '天的', '云', '彩', '。', '\\n', '\\n', '那', '河', '�', '�', '的', '金', '柳', '，', '\\n', '是', '夕', '阳', '中的', '新', '娘', '；', '\\n', '波', '光', '里的', '艳', '影', '，', '\\n', '在', '我的', '心', '头', '荡', '�', '�', '。', '\\n', '\\n', '软', '泥', '上的', '青', '�', '�', '，', '\\n', '油', '油', '的', '在', '水', '底', '招', '摇', '；', '\\n', '在', '康', '河', '的', '柔', '波', '里', '，', '\\n', '我', '甘', '心', '做', '一条', '水', '草', '！', '\\n', '\\n', '那', '�', '�', '�', '�', '下', '的一', '�', '�', '，', '\\n', '不是', '清', '泉', '，', '是', '天', '上', '�', '�', '；', '\\n', '�', '�', '碎', '在', '浮', '�', '�', '间', '，', '\\n', '沉', '淀', '着', '彩', '�', '�', '似', '的', '梦', '。', '\\n', '\\n', '寻', '梦', '？', '撑', '一', '支', '长', '�', '�', '，', '\\n', '向', '青', '草', '更', '青', '处', '漫', '�', '�', '；', '\\n', '满', '载', '一', '船', '星', '辉', '，', '\\n', '在', '星', '辉', '斑', '�', '�', '里', '放', '歌', '。', '\\n', '\\n', '但', '我', '不能', '放', '歌', '，', '\\n', '悄', '悄', '是', '别', '离', '的', '�', '�', '�', '�', '；', '\\n', '夏', '虫', '也', '为', '我', '沉', '默', '，', '\\n', '沉', '默', '是', '今', '晚', '的', '康', '桥', '！', '\\n', '\\n', '悄', '悄', '的', '我', '走了', '，', '\\n', '正', '如', '我', '悄', '悄', '的', '来', '；', '\\n', '我', '挥', '一', '挥', '衣', '袖', '，', '\\n', '不', '带', '走', '一片', '云', '彩', '。']\n",
      "<｜begin▁of▁sentence｜>轻轻的我走了，\n",
      "正如我轻轻的来；\n",
      "我轻轻的招手，\n",
      "作别西天的云彩。\n",
      "\n",
      "那河畔的金柳，\n",
      "是夕阳中的新娘；\n",
      "波光里的艳影，\n",
      "在我的心头荡漾。\n",
      "\n",
      "软泥上的青荇，\n",
      "油油的在水底招摇；\n",
      "在康河的柔波里，\n",
      "我甘心做一条水草！\n",
      "\n",
      "那榆荫下的一潭，\n",
      "不是清泉，是天上虹；\n",
      "揉碎在浮藻间，\n",
      "沉淀着彩虹似的梦。\n",
      "\n",
      "寻梦？撑一支长篙，\n",
      "向青草更青处漫溯；\n",
      "满载一船星辉，\n",
      "在星辉斑斓里放歌。\n",
      "\n",
      "但我不能放歌，\n",
      "悄悄是别离的笙箫；\n",
      "夏虫也为我沉默，\n",
      "沉默是今晚的康桥！\n",
      "\n",
      "悄悄的我走了，\n",
      "正如我悄悄的来；\n",
      "我挥一挥衣袖，\n",
      "不带走一片云彩。\n"
     ]
    }
   ],
   "source": [
    "with open(\"再别康桥.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "print(text)\n",
    "tokens = tokenizer.encode(text)\n",
    "print([tokenizer.decode([t]) for t in tokens])\n",
    "print(tokenizer.decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
