{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4a4c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"  # small-ish\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebeafa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32013, 1367, 1248, 4986]\n",
      "<｜begin▁of▁sentence｜>你好世界\n",
      "['<｜begin▁of▁sentence｜>', '你', '好', '世界']\n"
     ]
    }
   ],
   "source": [
    "# Encode some text into tokens\n",
    "text = \"你好世界\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)\n",
    "\n",
    "# Decode back into text\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(decoded)\n",
    "\n",
    "# Inspect how it split the text\n",
    "print([tokenizer.decode([t]) for t in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202975f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<｜begin▁of▁sentence｜>', 'ä½ł', 'å¥½', 'ä¸ĸçķĮ']\n",
      "vocab size: 32000\n",
      "all special tokens: ['<｜begin▁of▁sentence｜>', '<｜end▁of▁sentence｜>']\n",
      "special tokens map: {'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<｜end▁of▁sentence｜>', 'pad_token': '<｜end▁of▁sentence｜>'}\n",
      "[1367, 1248, 4986]\n",
      "['你', '好', '世界']\n"
     ]
    }
   ],
   "source": [
    "# convert ids -> token strings\n",
    "tokens = [32013, 1367, 1248, 4986]\n",
    "print(tokenizer.convert_ids_to_tokens(tokens))\n",
    "\n",
    "# vocab size and specials\n",
    "print(\"vocab size:\", tokenizer.vocab_size)\n",
    "print(\"all special tokens:\", tokenizer.all_special_tokens)\n",
    "print(\"special tokens map:\", tokenizer.special_tokens_map)\n",
    "\n",
    "# encode without adding special tokens (if you don't want BOS/EOS)\n",
    "tokens = tokenizer.encode(\"你好世界\", add_special_tokens=False)\n",
    "print(tokens)\n",
    "\n",
    "# show ID -> decoding for each id (what you already saw)\n",
    "print([tokenizer.decode([t]) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01651da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token id: 32013 token str: <｜begin▁of▁sentence｜>\n",
      "token id: 4642 token str: 轻\n",
      "token id: 4642 token str: 轻\n",
      "token id: 337 token str: 的\n",
      "token id: 848 token str: 我\n",
      "token id: 26388 token str: 走了\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 2198 token str: 正\n",
      "token id: 1410 token str: 如\n",
      "token id: 848 token str: 我\n",
      "token id: 4642 token str: 轻\n",
      "token id: 4642 token str: 轻\n",
      "token id: 337 token str: 的\n",
      "token id: 908 token str: 来\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 848 token str: 我\n",
      "token id: 4642 token str: 轻\n",
      "token id: 4642 token str: 轻\n",
      "token id: 337 token str: 的\n",
      "token id: 6716 token str: 招\n",
      "token id: 1897 token str: 手\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 1147 token str: 作\n",
      "token id: 2501 token str: 别\n",
      "token id: 2787 token str: 西\n",
      "token id: 17881 token str: 天的\n",
      "token id: 5973 token str: 云\n",
      "token id: 7817 token str: 彩\n",
      "token id: 397 token str: 。\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 1865 token str: 那\n",
      "token id: 6017 token str: 河\n",
      "token id: 1911 token str: �\n",
      "token id: 229 token str: �\n",
      "token id: 337 token str: 的\n",
      "token id: 2146 token str: 金\n",
      "token id: 23385 token str: 柳\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 502 token str: 是\n",
      "token id: 25652 token str: 夕\n",
      "token id: 4611 token str: 阳\n",
      "token id: 5374 token str: 中的\n",
      "token id: 1399 token str: 新\n",
      "token id: 15743 token str: 娘\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 7204 token str: 波\n",
      "token id: 3372 token str: 光\n",
      "token id: 10897 token str: 里的\n",
      "token id: 20550 token str: 艳\n",
      "token id: 3103 token str: 影\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 608 token str: 在\n",
      "token id: 6885 token str: 我的\n",
      "token id: 1342 token str: 心\n",
      "token id: 2641 token str: 头\n",
      "token id: 17768 token str: 荡\n",
      "token id: 3022 token str: �\n",
      "token id: 122 token str: �\n",
      "token id: 397 token str: 。\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 8857 token str: 软\n",
      "token id: 15798 token str: 泥\n",
      "token id: 5805 token str: 上的\n",
      "token id: 5025 token str: 青\n",
      "token id: 2329 token str: �\n",
      "token id: 216 token str: �\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 4324 token str: 油\n",
      "token id: 4324 token str: 油\n",
      "token id: 337 token str: 的\n",
      "token id: 608 token str: 在\n",
      "token id: 1843 token str: 水\n",
      "token id: 4795 token str: 底\n",
      "token id: 6716 token str: 招\n",
      "token id: 20148 token str: 摇\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 608 token str: 在\n",
      "token id: 5074 token str: 康\n",
      "token id: 6017 token str: 河\n",
      "token id: 337 token str: 的\n",
      "token id: 14680 token str: 柔\n",
      "token id: 7204 token str: 波\n",
      "token id: 1673 token str: 里\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 848 token str: 我\n",
      "token id: 14829 token str: 甘\n",
      "token id: 1342 token str: 心\n",
      "token id: 2077 token str: 做\n",
      "token id: 17817 token str: 一条\n",
      "token id: 1843 token str: 水\n",
      "token id: 8150 token str: 草\n",
      "token id: 2149 token str: ！\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 1865 token str: 那\n",
      "token id: 6276 token str: �\n",
      "token id: 215 token str: �\n",
      "token id: 2329 token str: �\n",
      "token id: 104 token str: �\n",
      "token id: 1153 token str: 下\n",
      "token id: 4284 token str: 的一\n",
      "token id: 6348 token str: �\n",
      "token id: 242 token str: �\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 3580 token str: 不是\n",
      "token id: 3151 token str: 清\n",
      "token id: 15370 token str: 泉\n",
      "token id: 19385 token str: ，\n",
      "token id: 502 token str: 是\n",
      "token id: 1393 token str: 天\n",
      "token id: 812 token str: 上\n",
      "token id: 2616 token str: �\n",
      "token id: 117 token str: �\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 1611 token str: �\n",
      "token id: 218 token str: �\n",
      "token id: 17162 token str: 碎\n",
      "token id: 608 token str: 在\n",
      "token id: 17066 token str: 浮\n",
      "token id: 7447 token str: �\n",
      "token id: 119 token str: �\n",
      "token id: 1651 token str: 间\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 10780 token str: 沉\n",
      "token id: 26481 token str: 淀\n",
      "token id: 1643 token str: 着\n",
      "token id: 7817 token str: 彩\n",
      "token id: 2616 token str: �\n",
      "token id: 117 token str: �\n",
      "token id: 7078 token str: 似\n",
      "token id: 337 token str: 的\n",
      "token id: 8313 token str: 梦\n",
      "token id: 397 token str: 。\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 11490 token str: 寻\n",
      "token id: 8313 token str: 梦\n",
      "token id: 2215 token str: ？\n",
      "token id: 15404 token str: 撑\n",
      "token id: 503 token str: 一\n",
      "token id: 3769 token str: 支\n",
      "token id: 1591 token str: 长\n",
      "token id: 7745 token str: �\n",
      "token id: 234 token str: �\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 2643 token str: 向\n",
      "token id: 5025 token str: 青\n",
      "token id: 8150 token str: 草\n",
      "token id: 1791 token str: 更\n",
      "token id: 5025 token str: 青\n",
      "token id: 2689 token str: 处\n",
      "token id: 12719 token str: 漫\n",
      "token id: 3083 token str: �\n",
      "token id: 107 token str: �\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 4136 token str: 满\n",
      "token id: 7438 token str: 载\n",
      "token id: 503 token str: 一\n",
      "token id: 13505 token str: 船\n",
      "token id: 4714 token str: 星\n",
      "token id: 18305 token str: 辉\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 608 token str: 在\n",
      "token id: 4714 token str: 星\n",
      "token id: 18305 token str: 辉\n",
      "token id: 18372 token str: 斑\n",
      "token id: 595 token str: �\n",
      "token id: 228 token str: �\n",
      "token id: 1673 token str: 里\n",
      "token id: 2646 token str: 放\n",
      "token id: 8028 token str: 歌\n",
      "token id: 397 token str: 。\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 1699 token str: 但\n",
      "token id: 848 token str: 我\n",
      "token id: 4867 token str: 不能\n",
      "token id: 2646 token str: 放\n",
      "token id: 8028 token str: 歌\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 28366 token str: 悄\n",
      "token id: 28366 token str: 悄\n",
      "token id: 502 token str: 是\n",
      "token id: 2501 token str: 别\n",
      "token id: 4270 token str: 离\n",
      "token id: 337 token str: 的\n",
      "token id: 1514 token str: �\n",
      "token id: 234 token str: �\n",
      "token id: 1482 token str: �\n",
      "token id: 104 token str: �\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 9020 token str: 夏\n",
      "token id: 19142 token str: 虫\n",
      "token id: 1021 token str: 也\n",
      "token id: 772 token str: 为\n",
      "token id: 848 token str: 我\n",
      "token id: 10780 token str: 沉\n",
      "token id: 12854 token str: 默\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 10780 token str: 沉\n",
      "token id: 12854 token str: 默\n",
      "token id: 502 token str: 是\n",
      "token id: 2919 token str: 今\n",
      "token id: 6046 token str: 晚\n",
      "token id: 337 token str: 的\n",
      "token id: 5074 token str: 康\n",
      "token id: 10626 token str: 桥\n",
      "token id: 2149 token str: ！\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 28366 token str: 悄\n",
      "token id: 28366 token str: 悄\n",
      "token id: 337 token str: 的\n",
      "token id: 848 token str: 我\n",
      "token id: 26388 token str: 走了\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 2198 token str: 正\n",
      "token id: 1410 token str: 如\n",
      "token id: 848 token str: 我\n",
      "token id: 28366 token str: 悄\n",
      "token id: 28366 token str: 悄\n",
      "token id: 337 token str: 的\n",
      "token id: 908 token str: 来\n",
      "token id: 1989 token str: ；\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 848 token str: 我\n",
      "token id: 9706 token str: 挥\n",
      "token id: 503 token str: 一\n",
      "token id: 9706 token str: 挥\n",
      "token id: 7096 token str: 衣\n",
      "token id: 29452 token str: 袖\n",
      "token id: 19385 token str: ，\n",
      "token id: 185 token str: \n",
      "\n",
      "token id: 570 token str: 不\n",
      "token id: 2950 token str: 带\n",
      "token id: 3366 token str: 走\n",
      "token id: 25190 token str: 一片\n",
      "token id: 5973 token str: 云\n",
      "token id: 7817 token str: 彩\n",
      "token id: 397 token str: 。\n",
      "length of tokens: 261\n"
     ]
    }
   ],
   "source": [
    "with open(\"再别康桥.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "tokens = tokenizer.encode(text)\n",
    "for t in tokens:\n",
    "    print(\"token id:\", t, \"token str:\", tokenizer.decode([t]))\n",
    "print(\"length of tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9bf6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_gemma = AutoTokenizer.from_pretrained(\"google/gemma-2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a26b5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 256000\n",
      "token id: 2 token str: <bos>\n",
      "token id: 79424 token str: 轻轻\n",
      "token id: 153698 token str: 的我\n",
      "token id: 44913 token str: 走了\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 161181 token str: 正如\n",
      "token id: 235509 token str: 我\n",
      "token id: 79424 token str: 轻轻\n",
      "token id: 235370 token str: 的\n",
      "token id: 235547 token str: 来\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235509 token str: 我\n",
      "token id: 79424 token str: 轻轻\n",
      "token id: 235370 token str: 的\n",
      "token id: 237219 token str: 招\n",
      "token id: 235616 token str: 手\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235591 token str: 作\n",
      "token id: 236273 token str: 别\n",
      "token id: 235990 token str: 西\n",
      "token id: 55881 token str: 天的\n",
      "token id: 236537 token str: 云\n",
      "token id: 236729 token str: 彩\n",
      "token id: 235362 token str: 。\n",
      "token id: 109 token str: \n",
      "\n",
      "\n",
      "token id: 235779 token str: 那\n",
      "token id: 236811 token str: 河\n",
      "token id: 241225 token str: 畔\n",
      "token id: 172765 token str: 的金\n",
      "token id: 238110 token str: 柳\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235427 token str: 是\n",
      "token id: 237957 token str: 夕\n",
      "token id: 236899 token str: 阳\n",
      "token id: 14520 token str: 中的\n",
      "token id: 208564 token str: 新娘\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 236715 token str: 波\n",
      "token id: 235973 token str: 光\n",
      "token id: 32702 token str: 里的\n",
      "token id: 239698 token str: 艳\n",
      "token id: 236006 token str: 影\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 229749 token str: 在我的\n",
      "token id: 194252 token str: 心头\n",
      "token id: 239710 token str: 荡\n",
      "token id: 242262 token str: 漾\n",
      "token id: 235362 token str: 。\n",
      "token id: 109 token str: \n",
      "\n",
      "\n",
      "token id: 237153 token str: 软\n",
      "token id: 238542 token str: 泥\n",
      "token id: 16774 token str: 上的\n",
      "token id: 236348 token str: 青\n",
      "token id: 249534 token str: 荇\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 236541 token str: 油\n",
      "token id: 236541 token str: 油\n",
      "token id: 219935 token str: 的在\n",
      "token id: 235773 token str: 水\n",
      "token id: 236507 token str: 底\n",
      "token id: 237219 token str: 招\n",
      "token id: 237996 token str: 摇\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235473 token str: 在\n",
      "token id: 236860 token str: 康\n",
      "token id: 236811 token str: 河\n",
      "token id: 235370 token str: 的\n",
      "token id: 237640 token str: 柔\n",
      "token id: 236715 token str: 波\n",
      "token id: 235792 token str: 里\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235509 token str: 我\n",
      "token id: 237599 token str: 甘\n",
      "token id: 235675 token str: 心\n",
      "token id: 235928 token str: 做\n",
      "token id: 62421 token str: 一条\n",
      "token id: 235773 token str: 水\n",
      "token id: 236726 token str: 草\n",
      "token id: 235482 token str: ！\n",
      "token id: 109 token str: \n",
      "\n",
      "\n",
      "token id: 235779 token str: 那\n",
      "token id: 243210 token str: 榆\n",
      "token id: 243665 token str: 荫\n",
      "token id: 235543 token str: 下\n",
      "token id: 13824 token str: 的一\n",
      "token id: 240701 token str: 潭\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 8835 token str: 不是\n",
      "token id: 236056 token str: 清\n",
      "token id: 237228 token str: 泉\n",
      "token id: 235365 token str: ，\n",
      "token id: 235427 token str: 是\n",
      "token id: 197162 token str: 天上\n",
      "token id: 239106 token str: 虹\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 240272 token str: 揉\n",
      "token id: 238329 token str: 碎\n",
      "token id: 235473 token str: 在\n",
      "token id: 237828 token str: 浮\n",
      "token id: 240888 token str: 藻\n",
      "token id: 235885 token str: 间\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 237473 token str: 沉\n",
      "token id: 241169 token str: 淀\n",
      "token id: 235668 token str: 着\n",
      "token id: 170684 token str: 彩虹\n",
      "token id: 95681 token str: 似的\n",
      "token id: 237278 token str: 梦\n",
      "token id: 235362 token str: 。\n",
      "token id: 109 token str: \n",
      "\n",
      "\n",
      "token id: 238166 token str: 寻\n",
      "token id: 237278 token str: 梦\n",
      "token id: 235544 token str: ？\n",
      "token id: 239694 token str: 撑\n",
      "token id: 193584 token str: 一支\n",
      "token id: 236045 token str: 长\n",
      "token id: 249697 token str: 篙\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235983 token str: 向\n",
      "token id: 236348 token str: 青\n",
      "token id: 236726 token str: 草\n",
      "token id: 235858 token str: 更\n",
      "token id: 236348 token str: 青\n",
      "token id: 236327 token str: 处\n",
      "token id: 236613 token str: 漫\n",
      "token id: 241691 token str: 溯\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 236762 token str: 满\n",
      "token id: 236640 token str: 载\n",
      "token id: 235411 token str: 一\n",
      "token id: 237166 token str: 船\n",
      "token id: 236181 token str: 星\n",
      "token id: 239392 token str: 辉\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235473 token str: 在\n",
      "token id: 236181 token str: 星\n",
      "token id: 239392 token str: 辉\n",
      "token id: 239309 token str: 斑\n",
      "token id: 245396 token str: 斓\n",
      "token id: 235792 token str: 里\n",
      "token id: 236026 token str: 放\n",
      "token id: 236481 token str: 歌\n",
      "token id: 235362 token str: 。\n",
      "token id: 109 token str: \n",
      "\n",
      "\n",
      "token id: 108698 token str: 但我\n",
      "token id: 14504 token str: 不能\n",
      "token id: 236026 token str: 放\n",
      "token id: 236481 token str: 歌\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 149757 token str: 悄悄\n",
      "token id: 235427 token str: 是\n",
      "token id: 236273 token str: 别\n",
      "token id: 236621 token str: 离\n",
      "token id: 235370 token str: 的\n",
      "token id: 241296 token str: 笙\n",
      "token id: 244435 token str: 箫\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 236535 token str: 夏\n",
      "token id: 237960 token str: 虫\n",
      "token id: 235623 token str: 也\n",
      "token id: 235640 token str: 为\n",
      "token id: 235509 token str: 我\n",
      "token id: 82018 token str: 沉默\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 82018 token str: 沉默\n",
      "token id: 235427 token str: 是\n",
      "token id: 145728 token str: 今晚\n",
      "token id: 235370 token str: 的\n",
      "token id: 236860 token str: 康\n",
      "token id: 238699 token str: 桥\n",
      "token id: 235482 token str: ！\n",
      "token id: 109 token str: \n",
      "\n",
      "\n",
      "token id: 149757 token str: 悄悄\n",
      "token id: 153698 token str: 的我\n",
      "token id: 44913 token str: 走了\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 161181 token str: 正如\n",
      "token id: 235509 token str: 我\n",
      "token id: 149757 token str: 悄悄\n",
      "token id: 235370 token str: 的\n",
      "token id: 235547 token str: 来\n",
      "token id: 236334 token str: ；\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235509 token str: 我\n",
      "token id: 238350 token str: 挥\n",
      "token id: 235411 token str: 一\n",
      "token id: 238350 token str: 挥\n",
      "token id: 236524 token str: 衣\n",
      "token id: 237785 token str: 袖\n",
      "token id: 235365 token str: ，\n",
      "token id: 108 token str: \n",
      "\n",
      "token id: 235441 token str: 不\n",
      "token id: 236373 token str: 带\n",
      "token id: 236142 token str: 走\n",
      "token id: 72577 token str: 一片\n",
      "token id: 236537 token str: 云\n",
      "token id: 236729 token str: 彩\n",
      "token id: 235362 token str: 。\n",
      "length of tokens: 217\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer_gemma.encode(text)\n",
    "print(\"vocab size:\", tokenizer_gemma.vocab_size)\n",
    "for t in tokens:\n",
    "    print(\"token id:\", t, \"token str:\", tokenizer_gemma.decode([t]))\n",
    "print(\"length of tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466630c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Example with DeepSeek\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Your input so far (the context)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m inputs = \u001b[43mtokenizer\u001b[49m(text, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Example with DeepSeek\n",
    "\n",
    "# Your input so far (the context)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Run the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get logits for the next token prediction\n",
    "logits = outputs.logits[0, -1, :]  # last token's logits\n",
    "print(logits.shape)  # should be (vocab_size,)\n",
    "# print top 10 logits\n",
    "print(torch.topk(logits, 10))\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Find top probable tokens\n",
    "top_probs, top_indices = torch.topk(probs, 10)\n",
    "for i, p in zip(top_indices, top_probs):\n",
    "    print(f\"{tokenizer.decode(i)}: {p.item():.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7410421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xfc\\xd1\\xa7'\n"
     ]
    }
   ],
   "source": [
    "# test on this alphabet:\n",
    "alphabet = [\"<EOF>\", \"a\", \"e\", \"i\", \"o\", \"u\"]\n",
    "probabilities = [0.1, 0.2, 0.3, 0.15, 0.15, 0.1]\n",
    "symbol_to_id = {s: i for i, s in enumerate(alphabet)}\n",
    "message = [\"u\", \"o\", \"o\", \"e\", \"u\", \"i\", \"a\", \"<EOF>\"]\n",
    "\n",
    "import encoder\n",
    "\n",
    "enc = encoder.Encoder()\n",
    "for symbol in message:\n",
    "    id = symbol_to_id[symbol]\n",
    "    enc.encode(symbol, id, probabilities)\n",
    "print(enc.get_encoded_bytes())  # print as integer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48948b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model counts (first few): [6554, 13107, 19661, 9830, 9830, 6553] sum: 65535\n",
      "cum_desc[0] (total): 65535\n",
      "Encoded bytes (hex): 5400\n",
      "Encoded bitstring (bytes*8 bits shown): 0101010000000000\n",
      "After encoding: encoder L= 0 R= 32768 bits_waiting= 0\n",
      "Decoder primed: L= 0 D= 21504 R= 65536\n",
      "decoded step 0: idx=3, symbol=o\n",
      "decoded step 1: idx=2, symbol=i\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Decoder invalid target=-15971 at step 2 (L=58979, D=43008, R=19661)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 140\u001b[39m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDecoder: invalid R=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m at step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m0\u001b[39m <= target < R):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDecoder invalid target=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m at step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (L=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoder_dec.L\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, D=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoder_dec.D\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, R=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# textbook mapping to integer count\u001b[39;00m\n\u001b[32m    143\u001b[39m count = ((target + \u001b[32m1\u001b[39m) * total - \u001b[32m1\u001b[39m) // R\n",
      "\u001b[31mRuntimeError\u001b[39m: Decoder invalid target=-15971 at step 2 (L=58979, D=43008, R=19661)"
     ]
    }
   ],
   "source": [
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from arithmetic_coding import Coder\n",
    "from bitReadWrite import BitReader, BitWriter\n",
    "\n",
    "# Jupyter test cell: end-to-end encode/decode using your Coder / Encoder / Decoder classes.\n",
    "# Paste this cell below where you have defined or imported Coder, Encoder, Decoder.\n",
    "\n",
    "from typing import List\n",
    "import bisect\n",
    "\n",
    "# -------------------------\n",
    "# Small helpers (copy into cell so test is self-contained)\n",
    "# -------------------------\n",
    "def probs_to_counts_largest_remainder(probs: List[float], slots: int) -> List[int]:\n",
    "    \"\"\"Deterministic mapping of floats -> integer counts summing to exactly 'slots'.\"\"\"\n",
    "    if len(probs) == 0:\n",
    "        raise ValueError(\"Empty probability list\")\n",
    "    raw = [float(p) for p in probs]\n",
    "    s = sum(raw)\n",
    "    if s <= 0:\n",
    "        raise ValueError(\"Probabilities must sum to > 0\")\n",
    "    scaled = [r * slots / s for r in raw]\n",
    "    floors = [int(x) for x in scaled]\n",
    "    remainder = slots - sum(floors)\n",
    "    fracs = sorted(((i, scaled[i] - floors[i]) for i in range(len(probs))),\n",
    "                   key=lambda x: x[1], reverse=True)\n",
    "    i = 0\n",
    "    while remainder > 0 and i < len(fracs):\n",
    "        floors[fracs[i][0]] += 1\n",
    "        remainder -= 1\n",
    "        i += 1\n",
    "    # safety: if a nonzero prob got zero count we must increase slots/precision\n",
    "    for p, c in zip(raw, floors):\n",
    "        if p > 0.0 and c == 0:\n",
    "            raise ValueError(\"Precision too low: a nonzero probability mapped to 0 count\")\n",
    "    return floors\n",
    "\n",
    "def counts_to_cum_desc(counts: List[int]) -> List[int]:\n",
    "    \"\"\"Convert counts -> descending cumulative array used by CACM: cum_desc[0]=total, cum_desc[-1]=0.\"\"\"\n",
    "    total = sum(counts)\n",
    "    cum = [total]\n",
    "    s = 0\n",
    "    for c in counts:\n",
    "        s += c\n",
    "        cum.append(total - s)\n",
    "    return cum  # length n+1, cum[0]=total, cum[n]=0\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Test config (your example)\n",
    "# -------------------------\n",
    "alphabet = ['a', 'e', 'i', 'o', 'u', '<EOF>']\n",
    "probs = [0.1, 0.2, 0.3, 0.15, 0.15, 0.1]\n",
    "message = [\"u\", \"o\", \"o\", \"e\", \"u\", \"i\", \"a\", \"<EOF>\"]\n",
    "\n",
    "# -------------------------\n",
    "# Ensure Coder / Encoder / Decoder exist (user's classes)\n",
    "# -------------------------\n",
    "if 'Coder' not in globals():\n",
    "    raise RuntimeError(\"Coder class not found in notebook scope. Define/import Coder before running this cell.\")\n",
    "if 'Encoder' not in globals():\n",
    "    raise RuntimeError(\"Encoder class not found in notebook scope. Define/import Encoder before running this cell.\")\n",
    "if 'Decoder' not in globals():\n",
    "    raise RuntimeError(\"Decoder class not found in notebook scope. Define/import Decoder before running this cell.\")\n",
    "\n",
    "# -------------------------\n",
    "# Build model counts and cumulative (descending) expected by textbook wrappers\n",
    "# -------------------------\n",
    "# instantiate a temporary Coder to choose slots consistent with its precision\n",
    "temp_coder = globals()['Coder'](b=16)   # choose b=16 for compactness; increase to 32 to match paper if desired\n",
    "# slots: choose temp_coder.tb ( = 2^b - 1 ) so total fits inside coder precision\n",
    "slots = temp_coder.tb\n",
    "counts = probs_to_counts_largest_remainder(probs, slots)\n",
    "cum_desc = counts_to_cum_desc(counts)\n",
    "\n",
    "print(\"Model counts (first few):\", counts[:6], \"sum:\", sum(counts))\n",
    "print(\"cum_desc[0] (total):\", cum_desc[0])\n",
    "\n",
    "# -------------------------\n",
    "# ENCODING (use Coder.storeRegion directly to avoid wrapper convention mismatch)\n",
    "# -------------------------\n",
    "coder_enc = globals()['Coder'](b=16)\n",
    "bw = BitWriter()\n",
    "\n",
    "# prime the encoder (your Coder method is start_encode)\n",
    "if hasattr(coder_enc, 'start_encode'):\n",
    "    coder_enc.start_encode(bw)\n",
    "else:\n",
    "    raise RuntimeError(\"Coder missing start_encode; adapt call to your API.\")\n",
    "\n",
    "sym_to_idx = {s: i for i, s in enumerate(alphabet)}\n",
    "\n",
    "# perform symbol-by-symbol encoding using textbook descending cum_desc convention\n",
    "for sym in message:\n",
    "    idx = sym_to_idx[sym]\n",
    "    l = cum_desc[idx + 1]\n",
    "    h = cum_desc[idx]\n",
    "    # call the exact method you defined earlier\n",
    "    coder_enc.storeRegion(l, h)\n",
    "\n",
    "# finalize encoder and flush bit-writer\n",
    "coder_enc.finish_encode()\n",
    "bw.flush(padbit=0)   # your BitWriter implementation expects padbit param\n",
    "encoded_bytes = bw.getvalue()\n",
    "print(\"Encoded bytes (hex):\", encoded_bytes.hex())\n",
    "print(\"Encoded bitstring (bytes*8 bits shown):\", ''.join(f'{b:08b}' for b in encoded_bytes))\n",
    "\n",
    "# diagnostic: print coder_enc internal state after encoding finishes\n",
    "print(\"After encoding: encoder L=\", coder_enc.L, \"R=\", coder_enc.R, \"bits_waiting=\", coder_enc.bits_waiting)\n",
    "# prime a new decoder and print its initial state\n",
    "coder_dec = globals()['Coder'](b=16)\n",
    "coder_dec.start_decode(BitReader(encoded_bytes))\n",
    "print(\"Decoder primed: L=\", coder_dec.L, \"D=\", coder_dec.D, \"R=\", coder_dec.R)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# DECODING (direct Coder usage — uses attributes L,R,D and loadRegion)\n",
    "# -------------------------\n",
    "coder_dec = globals()['Coder'](b=16)\n",
    "br = BitReader(encoded_bytes)\n",
    "\n",
    "# prime decoder\n",
    "if hasattr(coder_dec, 'start_decode'):\n",
    "    coder_dec.start_decode(br)\n",
    "else:\n",
    "    raise RuntimeError(\"Coder missing start_decode; adapt call to your API.\")\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(message)):\n",
    "    # read the current integer range and target using the Coder attributes\n",
    "    R = coder_dec.R\n",
    "    target = coder_dec.D - coder_dec.L\n",
    "    total = cum_desc[0]\n",
    "\n",
    "    # defensive checks\n",
    "    if R <= 0:\n",
    "        raise RuntimeError(f\"Decoder: invalid R={R} at step {i}\")\n",
    "    if not (0 <= target < R):\n",
    "        raise RuntimeError(f\"Decoder invalid target={target} at step {i} (L={coder_dec.L}, D={coder_dec.D}, R={R})\")\n",
    "\n",
    "    # textbook mapping to integer count\n",
    "    count = ((target + 1) * total - 1) // R\n",
    "\n",
    "    # find symbol s with cum_desc[s+1] <= count < cum_desc[s]\n",
    "    s_found = None\n",
    "    for s in range(len(cum_desc) - 1):\n",
    "        if cum_desc[s + 1] <= count < cum_desc[s]:\n",
    "            s_found = s\n",
    "            break\n",
    "    if s_found is None:\n",
    "        s_found = len(cum_desc) - 2\n",
    "\n",
    "    # update coder state by loading the symbol's region\n",
    "    l = cum_desc[s_found + 1]\n",
    "    h = cum_desc[s_found]\n",
    "    coder_dec.loadRegion(l, h)\n",
    "\n",
    "    decoded.append(alphabet[s_found])\n",
    "    print(f\"decoded step {i}: idx={s_found}, symbol={alphabet[s_found]}\")\n",
    "\n",
    "print(\"Decoded message:\", decoded)\n",
    "print(\"Round-trip OK?\", decoded == message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99416669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts: [6554, 13107, 19661, 9830, 9830, 6553]\n",
      "cum_desc: [65535, 58981, 45874, 26213, 16383, 6553, 0]\n",
      "encoded message length: 8\n",
      "\n",
      "ENCODING: record intervals (before renorm)\n",
      "enc[0] 'u': lower=49152 upper=58982 -> new_low=49152 new_high=58981  (post-renorm L=32768 R=19660 bits_waiting=0)\n",
      "enc[1] 'o': lower=39322 upper=49152 -> new_low=44564 new_high=47512  (post-renorm L=28832 R=23592 bits_waiting=0)\n",
      "enc[2] 'o': lower=39322 upper=49152 -> new_low=42987 new_high=46525  (post-renorm L=16216 R=28312 bits_waiting=0)\n",
      "enc[3] 'e': lower=6554 upper=19661 -> new_low=19047 new_high=24708  (post-renorm L=10652 R=22648 bits_waiting=0)\n",
      "enc[4] 'u': lower=49152 upper=58982 -> new_low=27638 new_high=31034  (post-renorm L=24496 R=27176 bits_waiting=0)\n",
      "enc[5] 'i': lower=19661 upper=39322 -> new_low=32649 new_high=40801  (post-renorm L=32292 R=32612 bits_waiting=2)\n",
      "enc[6] 'a': lower=0 upper=6554 -> new_low=32292 new_high=35552  (post-renorm L=28960 R=26088 bits_waiting=5)\n",
      "enc[7] '<EOF>': lower=58982 upper=65535 -> new_low=52439 new_high=55047  (post-renorm L=26296 R=20872 bits_waiting=0)\n",
      "\n",
      "Encoded bytes (hex): dab828\n",
      "Encoded bitstring: 110110101011100000101000\n",
      "encoder final L,R: 0 32768\n",
      "\n",
      "DECODING: compare intervals vs encoder records\n",
      "\n",
      "step 0: before selection: L=0 D=55992 R=65536\n",
      "  candidate intervals (s, sym, new_low, new_high):\n",
      "    (0, 'a', 0, 6553)\n",
      "    (1, 'e', 6554, 19660)\n",
      "    (2, 'i', 19661, 39321)\n",
      "    (3, 'o', 39322, 49151)\n",
      "    (4, 'u', 49152, 58981)\n",
      "    (5, '<EOF>', 58982, 65535)\n",
      "  DECODER selects s=4 sym='u' interval=(49152, 58981)\n",
      "  ENCODER recorded interval: (49152, 58981, 'u')\n",
      "  accepted symbol 'u', decoder after renorm: L=32768, D=46448, R=19660\n",
      "\n",
      "step 1: before selection: L=32768 D=46448 R=19660\n",
      "  candidate intervals (s, sym, new_low, new_high):\n",
      "    (0, 'a', 32768, 34733)\n",
      "    (1, 'e', 34734, 38665)\n",
      "    (2, 'i', 38666, 44563)\n",
      "    (3, 'o', 44564, 47512)\n",
      "    (4, 'u', 47513, 50461)\n",
      "    (5, '<EOF>', 50462, 52427)\n",
      "  DECODER selects s=3 sym='o' interval=(44564, 47512)\n",
      "  ENCODER recorded interval: (44564, 47512, 'o')\n",
      "  accepted symbol 'o', decoder after renorm: L=28832, D=43906, R=23592\n",
      "\n",
      "step 2: before selection: L=28832 D=43906 R=23592\n",
      "  candidate intervals (s, sym, new_low, new_high):\n",
      "    (0, 'a', 28832, 31190)\n",
      "    (1, 'e', 31191, 35908)\n",
      "    (2, 'i', 35909, 42986)\n",
      "    (3, 'o', 42987, 46525)\n",
      "    (4, 'u', 46526, 50063)\n",
      "    (5, '<EOF>', 50064, 52423)\n",
      "  DECODER selects s=3 sym='o' interval=(42987, 46525)\n",
      "  ENCODER recorded interval: (42987, 46525, 'o')\n",
      "  accepted symbol 'o', decoder after renorm: L=16216, D=23572, R=28312\n",
      "\n",
      "step 3: before selection: L=16216 D=23572 R=28312\n",
      "  candidate intervals (s, sym, new_low, new_high):\n",
      "    (0, 'a', 16216, 19046)\n",
      "    (1, 'e', 19047, 24708)\n",
      "    (2, 'i', 24709, 33202)\n",
      "    (3, 'o', 33203, 37449)\n",
      "    (4, 'u', 37450, 41696)\n",
      "    (5, '<EOF>', 41697, 44527)\n",
      "  DECODER selects s=1 sym='e' interval=(19047, 24708)\n",
      "  ENCODER recorded interval: (19047, 24708, 'e')\n",
      "  accepted symbol 'e', decoder after renorm: L=10652, D=28752, R=22648\n",
      "\n",
      "step 4: before selection: L=10652 D=28752 R=22648\n",
      "  candidate intervals (s, sym, new_low, new_high):\n",
      "    (0, 'a', 10652, 12915)\n",
      "    (1, 'e', 12916, 17445)\n",
      "    (2, 'i', 17446, 24240)\n",
      "    (3, 'o', 24241, 27637)\n",
      "    (4, 'u', 27638, 31034)\n",
      "    (5, '<EOF>', 31035, 33299)\n",
      "  DECODER selects s=4 sym='u' interval=(27638, 31034)\n",
      "  ENCODER recorded interval: (27638, 31034, 'u')\n",
      "  accepted symbol 'u', decoder after renorm: L=24496, D=33408, R=27176\n",
      "\n",
      "step 5: before selection: L=24496 D=33408 R=27176\n",
      "  candidate intervals (s, sym, new_low, new_high):\n",
      "    (0, 'a', 24496, 27212)\n",
      "    (1, 'e', 27213, 32648)\n",
      "    (2, 'i', 32649, 40801)\n",
      "    (3, 'o', 40802, 44877)\n",
      "    (4, 'u', 44878, 48953)\n",
      "    (5, '<EOF>', 48954, 51671)\n",
      "  DECODER selects s=2 sym='i' interval=(32649, 40801)\n",
      "  ENCODER recorded interval: (32649, 40801, 'i')\n",
      "  accepted symbol 'i', decoder after renorm: L=32292, D=35328, R=32612\n",
      "\n",
      "step 6: before selection: L=32292 D=35328 R=32612\n",
      "  candidate intervals (s, sym, new_low, new_high):\n",
      "    (0, 'a', 32292, 35552)\n",
      "    (1, 'e', 35553, 42074)\n",
      "    (2, 'i', 42075, 51858)\n",
      "    (3, 'o', 51859, 56750)\n",
      "    (4, 'u', 56751, 61642)\n",
      "    (5, '<EOF>', 61643, 64903)\n",
      "  DECODER selects s=0 sym='a' interval=(32292, 35552)\n",
      "  ENCODER recorded interval: (32292, 35552, 'a')\n",
      "  accepted symbol 'a', decoder after renorm: L=28960, D=53248, R=26088\n",
      "\n",
      "step 7: before selection: L=28960 D=53248 R=26088\n",
      "  candidate intervals (s, sym, new_low, new_high):\n",
      "    (0, 'a', 28960, 31567)\n",
      "    (1, 'e', 31568, 36785)\n",
      "    (2, 'i', 36786, 44612)\n",
      "    (3, 'o', 44613, 48525)\n",
      "    (4, 'u', 48526, 52438)\n",
      "    (5, '<EOF>', 52439, 55047)\n",
      "  DECODER selects s=5 sym='<EOF>' interval=(52439, 55047)\n",
      "  ENCODER recorded interval: (52439, 55047, '<EOF>')\n",
      "  accepted symbol '<EOF>', decoder after renorm: L=26296, D=32768, R=20872\n",
      "\n",
      "Decoded sequence: ['u', 'o', 'o', 'e', 'u', 'i', 'a', '<EOF>']\n",
      "Roundtrip OK?: True\n"
     ]
    }
   ],
   "source": [
    "# Deterministic interval-compare diagnostic\n",
    "from typing import List\n",
    "\n",
    "# reuse your model helpers\n",
    "def probs_to_counts_largest_remainder(probs: List[float], slots: int) -> List[int]:\n",
    "    raw = [float(p) for p in probs]; s = sum(raw)\n",
    "    scaled = [r * slots / s for r in raw]\n",
    "    floors = [int(x) for x in scaled]; remainder = slots - sum(floors)\n",
    "    fracs = sorted(((i, scaled[i] - floors[i]) for i in range(len(probs))),\n",
    "                   key=lambda x: x[1], reverse=True)\n",
    "    i = 0\n",
    "    while remainder > 0 and i < len(fracs):\n",
    "        floors[fracs[i][0]] += 1; remainder -= 1; i += 1\n",
    "    return floors\n",
    "\n",
    "def counts_to_cum_desc(counts: List[int]) -> List[int]:\n",
    "    total = sum(counts); cum=[total]; s=0\n",
    "    for c in counts:\n",
    "        s+=c; cum.append(total-s)\n",
    "    return cum\n",
    "\n",
    "# config\n",
    "alphabet = ['a','e','i','o','u','<EOF>']\n",
    "probs = [0.1,0.2,0.3,0.15,0.15,0.1]\n",
    "message = [\"u\",\"o\",\"o\",\"e\",\"u\",\"i\",\"a\",\"<EOF>\"]\n",
    "\n",
    "# model\n",
    "temp = Coder(b=16)\n",
    "slots = temp.tb\n",
    "counts = probs_to_counts_largest_remainder(probs, slots)\n",
    "cum_desc = counts_to_cum_desc(counts)\n",
    "total = cum_desc[0]\n",
    "print(\"counts:\", counts)\n",
    "print(\"cum_desc:\", cum_desc)\n",
    "print(\"encoded message length:\", len(message))\n",
    "\n",
    "# ENCODE and record encoder absolute intervals (new_low,new_high) BEFORE renormalisation\n",
    "coder_enc = Coder(b=16)\n",
    "bw = BitWriter()\n",
    "coder_enc.start_encode(bw)\n",
    "mask = coder_enc.mask\n",
    "\n",
    "enc_intervals = []  # list of tuples (new_low, new_high, symbol)\n",
    "print(\"\\nENCODING: record intervals (before renorm)\")\n",
    "for i,sym in enumerate(message):\n",
    "    idx = alphabet.index(sym)\n",
    "    l = cum_desc[idx+1]; h = cum_desc[idx]\n",
    "    lower = total - h; upper = total - l\n",
    "    rng = coder_enc.R\n",
    "    new_low = coder_enc.L + (rng * lower) // total\n",
    "    new_high = coder_enc.L + (rng * upper) // total - 1\n",
    "    enc_intervals.append((new_low, new_high, sym))\n",
    "    # apply to coder and renormalise (as you do)\n",
    "    coder_enc.L = new_low & mask\n",
    "    coder_enc.R = ((new_high - new_low + 1) & mask)\n",
    "    coder_enc._output_bits()\n",
    "    print(f\"enc[{i}] '{sym}': lower={lower} upper={upper} -> new_low={new_low} new_high={new_high}  (post-renorm L={coder_enc.L} R={coder_enc.R} bits_waiting={coder_enc.bits_waiting})\")\n",
    "\n",
    "coder_enc.finish_encode()\n",
    "bw.flush(padbit=0)\n",
    "encoded = bw.getvalue()\n",
    "print(\"\\nEncoded bytes (hex):\", encoded.hex())\n",
    "print(\"Encoded bitstring:\", ''.join(f\"{b:08b}\" for b in encoded))\n",
    "print(\"encoder final L,R:\", coder_enc.L, coder_enc.R)\n",
    "\n",
    "# DECODE and compare\n",
    "coder_dec = Coder(b=16)\n",
    "br = BitReader(encoded)\n",
    "coder_dec.start_decode(br)\n",
    "print(\"\\nDECODING: compare intervals vs encoder records\")\n",
    "decoded = []\n",
    "for i in range(len(message)):\n",
    "    L = coder_dec.L; R = coder_dec.R; D = coder_dec.D\n",
    "    print(f\"\\nstep {i}: before selection: L={L} D={D} R={R}\")\n",
    "    # compute every symbol's absolute interval as encoder would before renorm\n",
    "    found_s = None\n",
    "    candidate_intervals = []\n",
    "    for s in range(len(cum_desc)-1):\n",
    "        l = cum_desc[s+1]; h = cum_desc[s]\n",
    "        lower = total - h; upper = total - l\n",
    "        new_low_s = L + (R * lower) // total\n",
    "        new_high_s = L + (R * upper) // total - 1\n",
    "        candidate_intervals.append((s, alphabet[s], new_low_s, new_high_s))\n",
    "        if new_low_s <= D <= new_high_s:\n",
    "            found_s = s\n",
    "    # report candidates\n",
    "    print(\"  candidate intervals (s, sym, new_low, new_high):\")\n",
    "    for c in candidate_intervals:\n",
    "        print(\"   \", c)\n",
    "    if found_s is None:\n",
    "        print(\"  *** DECODER: no candidate interval contains D! ***\")\n",
    "    else:\n",
    "        print(f\"  DECODER selects s={found_s} sym='{alphabet[found_s]}' interval=({candidate_intervals[found_s][2]}, {candidate_intervals[found_s][3]})\")\n",
    "    # compare to encoder's recorded interval for this position\n",
    "    enc_low, enc_high, enc_sym = enc_intervals[i]\n",
    "    print(\"  ENCODER recorded interval:\", (enc_low, enc_high, enc_sym))\n",
    "    if found_s is None or enc_sym != alphabet[found_s] or (enc_low != candidate_intervals[found_s][2] or enc_high != candidate_intervals[found_s][3]):\n",
    "        print(\"\\nMISMATCH FOUND at step\", i)\n",
    "        print(\"  encoder interval vs decoder candidate:\")\n",
    "        print(\"   encoder:\", (enc_low, enc_high, enc_sym))\n",
    "        if found_s is None:\n",
    "            print(\"   decoder: no containing interval found (D not in any)\")\n",
    "        else:\n",
    "            print(\"   decoder candidate:\", candidate_intervals[found_s])\n",
    "        # show a bit more state and stop\n",
    "        print(\"  encoder post-renorm state for this step (L,R,bits_waiting):\", coder_enc.L, coder_enc.R, coder_enc.bits_waiting)\n",
    "        print(\"  decoder pre-renorm state for this step (L,D,R):\", L, D, R)\n",
    "        raise SystemExit(\"Stop: mismatch — paste the above and I'll recommend the single-line fix.\")\n",
    "    # If consistent, perform decoder load/renorm exactly as encoder did\n",
    "    # compute decoder's new_low/new_high identical to encoder's formulas\n",
    "    l = cum_desc[found_s+1]; h = cum_desc[found_s]\n",
    "    lower = total - h; upper = total - l\n",
    "    new_low = coder_dec.L + (coder_dec.R * lower) // total\n",
    "    new_high = coder_dec.L + (coder_dec.R * upper) // total - 1\n",
    "    coder_dec.L = new_low & coder_dec.mask\n",
    "    coder_dec.R = ((new_high - new_low + 1) & coder_dec.mask)\n",
    "    coder_dec._discard_bits()\n",
    "    decoded.append(alphabet[found_s])\n",
    "    print(f\"  accepted symbol '{alphabet[found_s]}', decoder after renorm: L={coder_dec.L}, D={coder_dec.D}, R={coder_dec.R}\")\n",
    "\n",
    "print(\"\\nDecoded sequence:\", decoded)\n",
    "print(\"Roundtrip OK?:\", decoded == message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7f3981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model counts: [6554, 13107, 19661, 9830, 9830, 6553] total: 65535\n",
      "Encoded bytes (hex): dab828\n",
      "Decoded: ['u', 'o', 'o', 'e', 'u', 'i', 'a', '<EOF>']\n",
      "Round-trip OK? True\n"
     ]
    }
   ],
   "source": [
    "from utils import probs_to_counts_largest_remainder, counts_to_cum_desc\n",
    "from arithmetic_coding import Coder\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from bitReadWrite import BitReader, BitWriter\n",
    "\n",
    "alphabet = ['a', 'e', 'i', 'o', 'u', '<EOF>']\n",
    "probs = [0.1, 0.2, 0.3, 0.15, 0.15, 0.1]\n",
    "message = [\"u\", \"o\", \"o\", \"e\", \"u\", \"i\", \"a\", \"<EOF>\"]\n",
    "\n",
    "# choose precision b=16 for quickness\n",
    "coder_temp = Coder(b=16)\n",
    "slots = coder_temp.tb  # = 2^b - 1\n",
    "counts = probs_to_counts_largest_remainder(probs, slots)\n",
    "cum_desc = counts_to_cum_desc(counts)\n",
    "\n",
    "print(\"Model counts:\", counts, \"total:\", cum_desc[0])\n",
    "\n",
    "# encode\n",
    "cw = BitWriter()\n",
    "coder_enc = Coder(b=16)\n",
    "enc = Encoder(coder_enc, cw)\n",
    "sym_to_idx = {s: i for i, s in enumerate(alphabet)}\n",
    "for sym in message:\n",
    "    idx = sym_to_idx[sym]\n",
    "    enc.encode_symbol(idx, cum_desc)\n",
    "enc.finish()\n",
    "cw.flush(padbit=0)\n",
    "encoded = cw.getvalue()\n",
    "print(\"Encoded bytes (hex):\", encoded.hex())\n",
    "\n",
    "# decode\n",
    "br = BitReader(encoded)\n",
    "coder_dec = Coder(b=16)\n",
    "dec = Decoder(coder_dec, br)\n",
    "decoded = []\n",
    "for _ in range(len(message)):\n",
    "    idx = dec.decode_symbol(cum_desc)\n",
    "    decoded.append(alphabet[idx])\n",
    "print(\"Decoded:\", decoded)\n",
    "print(\"Round-trip OK?\", decoded == message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
